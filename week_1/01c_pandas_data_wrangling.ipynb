{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e8287c6",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** ¬∑ *Intermediate AI & Data Science*\n",
    "### Week 01 ¬∑ Notebook 03 ‚Äî Data Wrangling & Transformation\n",
    "**Instructor:** Amir Charkhi  |  **Goal:** Master real-world data manipulation techniques.\n",
    "\n",
    "> Format: practical scenarios ‚Üí powerful pandas methods ‚Üí data ready for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a35e8bb",
   "metadata": {},
   "source": [
    "---\n",
    "## Real Data is Messy!\n",
    "Let's load and clean actual messy data - the skills you'll use every day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e4326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set display options for better visibility\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "\n",
    "print(\"Ready to wrangle! üõ†Ô∏è\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4f40a1",
   "metadata": {},
   "source": [
    "## 1. Reading Data from Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0037003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample CSV data (simulating a file)\n",
    "csv_data = \"\"\"order_id,customer_name,product,quantity,price,order_date\n",
    "1001,Alice Smith,Laptop,1,1200.00,2025-08-15\n",
    "1002,Bob Jones,Mouse,2,25.50,2025-08-15\n",
    "1003,Charlie Brown,,1,80.00,2025-08-16\n",
    "1004,Alice Smith,Monitor,1,,2025-08-16\n",
    "1005,Diana Prince,Keyboard,3,75.00,2025-08-17\n",
    "1006,,Webcam,1,120.00,2025-08-17\n",
    "1007,Bob Jones,Laptop,1,1200,2025-08-18\"\"\"\n",
    "\n",
    "# Save to file and read back\n",
    "with open('orders.csv', 'w') as f:\n",
    "    f.write(csv_data)\n",
    "\n",
    "# Read CSV with proper data types\n",
    "orders_df = pd.read_csv('orders.csv', parse_dates=['order_date'])\n",
    "print(\"Raw data from CSV:\")\n",
    "print(orders_df)\n",
    "print(f\"\\nData types:\")\n",
    "print(orders_df.dtypes)\n",
    "print(f\"\\nMissing values:\")\n",
    "print(orders_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741c1c71",
   "metadata": {},
   "source": [
    "## 2. Cleaning Missing and Incorrect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127850d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy for cleaning\n",
    "clean_df = orders_df.copy()\n",
    "\n",
    "# Handle missing customer names\n",
    "clean_df['customer_name'].fillna('Unknown Customer', inplace=True)\n",
    "\n",
    "# Handle missing products (look at other orders from same customer)\n",
    "clean_df.loc[2, 'product'] = 'Keyboard'  # Reasonable guess based on price\n",
    "\n",
    "# Handle missing prices (use average for that product)\n",
    "monitor_avg_price = 350.00  # Domain knowledge\n",
    "clean_df.loc[3, 'price'] = monitor_avg_price\n",
    "\n",
    "# Ensure price is float\n",
    "clean_df['price'] = pd.to_numeric(clean_df['price'], errors='coerce')\n",
    "\n",
    "# Calculate total\n",
    "clean_df['total'] = clean_df['quantity'] * clean_df['price']\n",
    "\n",
    "print(\"Cleaned data:\")\n",
    "print(clean_df)\n",
    "print(f\"\\nRevenue by customer:\")\n",
    "print(clean_df.groupby('customer_name')['total'].sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5265e60a",
   "metadata": {},
   "source": [
    "**Exercise 1 ‚Äî Data Quality Check (medium)**  \n",
    "Create a function that returns a data quality report: % complete, unique counts, and outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582c78e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f25302",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "def data_quality_report(df):\n",
    "    report = {}\n",
    "    \n",
    "    # Completeness\n",
    "    report['completeness'] = (1 - df.isnull().sum() / len(df)) * 100\n",
    "    \n",
    "    # Unique counts\n",
    "    report['unique_counts'] = df.nunique()\n",
    "    \n",
    "    # Outliers for numeric columns (using IQR)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    outliers = {}\n",
    "    for col in numeric_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers[col] = ((df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)).sum()\n",
    "    report['outliers'] = outliers\n",
    "    \n",
    "    return report\n",
    "\n",
    "quality_report = data_quality_report(clean_df)\n",
    "print(\"Data Quality Report:\")\n",
    "for key, value in quality_report.items():\n",
    "    print(f\"\\n{key}:\")\n",
    "    print(value)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd91af14",
   "metadata": {},
   "source": [
    "## 3. Merging and Joining DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93cc6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create related dataframes\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 4],\n",
    "    'name': ['Alice Smith', 'Bob Jones', 'Charlie Brown', 'Diana Prince'],\n",
    "    'city': ['Perth', 'Sydney', 'Melbourne', 'Brisbane'],\n",
    "    'member_since': ['2023-01-15', '2023-06-20', '2024-02-10', '2024-08-01']\n",
    "})\n",
    "\n",
    "orders = pd.DataFrame({\n",
    "    'order_id': [101, 102, 103, 104, 105],\n",
    "    'customer_id': [1, 2, 1, 3, 1],\n",
    "    'amount': [150, 250, 100, 300, 175],\n",
    "    'date': pd.date_range('2025-08-20', periods=5)\n",
    "})\n",
    "\n",
    "print(\"Customers:\")\n",
    "print(customers)\n",
    "print(\"\\nOrders:\")\n",
    "print(orders)\n",
    "\n",
    "# Merge dataframes\n",
    "merged = pd.merge(orders, customers, on='customer_id', how='left')\n",
    "print(\"\\nMerged data:\")\n",
    "print(merged)\n",
    "\n",
    "# Different join types\n",
    "print(\"\\nInner join (only matching):\")\n",
    "inner_join = pd.merge(orders, customers, on='customer_id', how='inner')\n",
    "print(f\"Rows: {len(inner_join)}\")\n",
    "\n",
    "print(\"\\nOuter join (all records):\")\n",
    "outer_join = pd.merge(orders, customers, on='customer_id', how='outer', indicator=True)\n",
    "print(outer_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac1848",
   "metadata": {},
   "source": [
    "## 4. Grouping and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf3b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sales data\n",
    "np.random.seed(42)\n",
    "sales = pd.DataFrame({\n",
    "    'date': pd.date_range('2025-08-01', periods=20),\n",
    "    'store': np.random.choice(['Store_A', 'Store_B', 'Store_C'], 20),\n",
    "    'product': np.random.choice(['Laptop', 'Phone', 'Tablet'], 20),\n",
    "    'quantity': np.random.randint(1, 10, 20),\n",
    "    'revenue': np.random.randint(100, 2000, 20)\n",
    "})\n",
    "\n",
    "print(\"Sales data:\")\n",
    "print(sales.head(10))\n",
    "\n",
    "# Group by store\n",
    "store_summary = sales.groupby('store').agg({\n",
    "    'quantity': 'sum',\n",
    "    'revenue': ['sum', 'mean', 'count']\n",
    "})\n",
    "print(\"\\nStore summary:\")\n",
    "print(store_summary)\n",
    "\n",
    "# Multiple grouping\n",
    "product_store = sales.groupby(['product', 'store'])['revenue'].sum().unstack(fill_value=0)\n",
    "print(\"\\nRevenue by product and store:\")\n",
    "print(product_store)\n",
    "\n",
    "# Add calculated columns\n",
    "sales['revenue_per_unit'] = sales['revenue'] / sales['quantity']\n",
    "sales['day_of_week'] = sales['date'].dt.day_name()\n",
    "\n",
    "# Group by day of week\n",
    "daily_pattern = sales.groupby('day_of_week')['revenue'].mean().round(2)\n",
    "print(\"\\nAverage revenue by day:\")\n",
    "print(daily_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c85c5e",
   "metadata": {},
   "source": [
    "**Exercise 2 ‚Äî Customer Segmentation (hard)**  \n",
    "Group customers by total spend into segments: VIP (>500), Regular (200-500), New (<200).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f39466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdbd6b9",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Calculate customer totals\n",
    "customer_totals = merged.groupby('name')['amount'].sum().reset_index()\n",
    "customer_totals.columns = ['customer', 'total_spend']\n",
    "\n",
    "# Create segments\n",
    "def segment_customer(spend):\n",
    "    if spend > 500: return 'VIP'\n",
    "    elif spend >= 200: return 'Regular'\n",
    "    else: return 'New'\n",
    "\n",
    "customer_totals['segment'] = customer_totals['total_spend'].apply(segment_customer)\n",
    "\n",
    "print(\"Customer segments:\")\n",
    "print(customer_totals.sort_values('total_spend', ascending=False))\n",
    "\n",
    "# Segment summary\n",
    "print(\"\\nSegment distribution:\")\n",
    "print(customer_totals['segment'].value_counts())\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f3255a",
   "metadata": {},
   "source": [
    "## 5. Pivoting and Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54870cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create long format data\n",
    "long_data = pd.DataFrame({\n",
    "    'date': pd.date_range('2025-08-01', periods=12),\n",
    "    'metric': ['Sales', 'Costs', 'Profit'] * 4,\n",
    "    'value': np.random.randint(1000, 5000, 12)\n",
    "})\n",
    "\n",
    "print(\"Long format:\")\n",
    "print(long_data)\n",
    "\n",
    "# Pivot to wide format\n",
    "wide_data = long_data.pivot(index='date', columns='metric', values='value')\n",
    "print(\"\\nWide format (pivoted):\")\n",
    "print(wide_data.head())\n",
    "\n",
    "# Melt back to long format\n",
    "melted = wide_data.reset_index().melt(id_vars='date', var_name='metric', value_name='amount')\n",
    "print(\"\\nMelted back to long:\")\n",
    "print(melted.head())\n",
    "\n",
    "# Pivot table with aggregation\n",
    "pivot_table = sales.pivot_table(\n",
    "    values='revenue',\n",
    "    index='store',\n",
    "    columns='product',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0,\n",
    "    margins=True\n",
    ")\n",
    "print(\"\\nPivot table with totals:\")\n",
    "print(pivot_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93813ac",
   "metadata": {},
   "source": [
    "## 6. String Operations and Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f70e311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create messy text data\n",
    "contacts = pd.DataFrame({\n",
    "    'name': ['  john smith  ', 'JANE DOE', 'Bob Johnson Jr.', 'alice wong'],\n",
    "    'email': ['John.Smith@GMAIL.com', 'jane@company.COM', 'bob@email.co', 'Alice@Email.net'],\n",
    "    'phone': ['0412-345-678', '(04) 9876 5432', '0401234567', '04 1111 2222']\n",
    "})\n",
    "\n",
    "print(\"Messy contact data:\")\n",
    "print(contacts)\n",
    "\n",
    "# Clean strings\n",
    "contacts['name_clean'] = contacts['name'].str.strip().str.title()\n",
    "contacts['email_clean'] = contacts['email'].str.lower()\n",
    "\n",
    "# Extract domain from email\n",
    "contacts['domain'] = contacts['email_clean'].str.split('@').str[1]\n",
    "\n",
    "# Standardize phone numbers\n",
    "contacts['phone_clean'] = contacts['phone'].str.replace(r'[^0-9]', '', regex=True)\n",
    "\n",
    "print(\"\\nCleaned contacts:\")\n",
    "print(contacts[['name_clean', 'email_clean', 'phone_clean', 'domain']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de98963a",
   "metadata": {},
   "source": [
    "## 7. Date and Time Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7b1b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series data\n",
    "dates = pd.date_range('2025-01-01', periods=100, freq='D')\n",
    "ts_data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'sales': np.random.randint(1000, 5000, 100) + np.sin(np.arange(100) * 2 * np.pi / 30) * 500\n",
    "})\n",
    "\n",
    "# Extract date components\n",
    "ts_data['year'] = ts_data['date'].dt.year\n",
    "ts_data['month'] = ts_data['date'].dt.month\n",
    "ts_data['day_of_week'] = ts_data['date'].dt.day_name()\n",
    "ts_data['week'] = ts_data['date'].dt.isocalendar().week\n",
    "\n",
    "print(\"Time series with date components:\")\n",
    "print(ts_data.head())\n",
    "\n",
    "# Resample to weekly\n",
    "weekly = ts_data.set_index('date')['sales'].resample('W').agg(['mean', 'sum', 'std'])\n",
    "print(\"\\nWeekly aggregation:\")\n",
    "print(weekly.head())\n",
    "\n",
    "# Rolling window calculations\n",
    "ts_data['rolling_mean_7d'] = ts_data['sales'].rolling(window=7).mean()\n",
    "ts_data['rolling_std_7d'] = ts_data['sales'].rolling(window=7).std()\n",
    "\n",
    "print(\"\\nWith rolling statistics:\")\n",
    "print(ts_data[['date', 'sales', 'rolling_mean_7d', 'rolling_std_7d']].tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cecfa82",
   "metadata": {},
   "source": [
    "**Exercise 3 ‚Äî Time Series Analysis (hard)**  \n",
    "Find the best and worst performing days of the week, and calculate week-over-week growth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f88bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f834bfd",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Best/worst days\n",
    "day_performance = ts_data.groupby('day_of_week')['sales'].agg(['mean', 'std'])\n",
    "day_performance = day_performance.sort_values('mean', ascending=False)\n",
    "print(\"Day of week performance:\")\n",
    "print(day_performance)\n",
    "print(f\"\\nBest day: {day_performance.index[0]}\")\n",
    "print(f\"Worst day: {day_performance.index[-1]}\")\n",
    "\n",
    "# Week-over-week growth\n",
    "weekly_sales = ts_data.groupby('week')['sales'].sum().reset_index()\n",
    "weekly_sales['wow_growth'] = weekly_sales['sales'].pct_change() * 100\n",
    "print(\"\\nWeek-over-week growth:\")\n",
    "print(weekly_sales.head(10))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a2e9d0",
   "metadata": {},
   "source": [
    "## 8. Mini-Challenges\n",
    "- **M1 (easy):** Create a function to detect duplicate rows based on subset of columns\n",
    "- **M2 (medium):** Implement a data validation function that checks data types and ranges\n",
    "- **M3 (hard):** Create a pipeline that cleans, transforms, and aggregates raw sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba28e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn - try the challenges!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4d4790",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solutions</b></summary>\n",
    "\n",
    "```python\n",
    "# M1 - Duplicate detection\n",
    "def find_duplicates(df, subset=None):\n",
    "    duplicates = df[df.duplicated(subset=subset, keep=False)]\n",
    "    return duplicates.sort_values(subset if subset else df.columns.tolist())\n",
    "\n",
    "# M2 - Data validation\n",
    "def validate_data(df, rules):\n",
    "    \"\"\"\n",
    "    rules = {\n",
    "        'column_name': {'type': str, 'min': 0, 'max': 100, 'required': True}\n",
    "    }\n",
    "    \"\"\"\n",
    "    issues = []\n",
    "    for col, rule in rules.items():\n",
    "        if col not in df.columns and rule.get('required'):\n",
    "            issues.append(f\"Missing required column: {col}\")\n",
    "            continue\n",
    "        \n",
    "        if 'type' in rule:\n",
    "            wrong_type = df[col].apply(lambda x: not isinstance(x, rule['type']))\n",
    "            if wrong_type.any():\n",
    "                issues.append(f\"{col}: {wrong_type.sum()} type mismatches\")\n",
    "        \n",
    "        if 'min' in rule:\n",
    "            below_min = df[col] < rule['min']\n",
    "            if below_min.any():\n",
    "                issues.append(f\"{col}: {below_min.sum()} values below {rule['min']}\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "# M3 - Data pipeline\n",
    "def sales_pipeline(raw_df):\n",
    "    # Clean\n",
    "    df = raw_df.copy()\n",
    "    df = df.dropna(subset=['product', 'revenue'])\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Transform\n",
    "    df['month'] = df['date'].dt.to_period('M')\n",
    "    df['revenue_category'] = pd.cut(df['revenue'], \n",
    "                                     bins=[0, 500, 1000, float('inf')],\n",
    "                                     labels=['Low', 'Medium', 'High'])\n",
    "    \n",
    "    # Aggregate\n",
    "    summary = df.groupby(['month', 'product']).agg({\n",
    "        'revenue': ['sum', 'mean', 'count'],\n",
    "        'quantity': 'sum'\n",
    "    }).round(2)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Test pipeline\n",
    "result = sales_pipeline(sales)\n",
    "print(\"Pipeline output:\")\n",
    "print(result.head())\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7840780f",
   "metadata": {},
   "source": [
    "## Wrap-Up\n",
    "‚úÖ You can read and clean messy real-world data  \n",
    "‚úÖ You mastered merging, grouping, and pivoting  \n",
    "‚úÖ You can handle dates, strings, and missing values  \n",
    "\n",
    "**Next:** EDA - Exploring and understanding your cleaned data!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
