{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e8287c6",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** · *Intermediate AI & Data Science*\n",
    "### Week 01 · Notebook 04 — Exploratory Data Analysis (EDA)\n",
    "**Instructor:** Amir Charkhi  |  **Goal:** Discover patterns, anomalies, and insights in real data.\n",
    "\n",
    "> Format: systematic exploration → visualization preview → statistical insights → storytelling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a35e8bb",
   "metadata": {},
   "source": [
    "---\n",
    "## EDA: The Detective Work of Data Science\n",
    "Before modeling or visualization, we need to understand our data deeply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e4326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Setup\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 2)\n",
    "\n",
    "print(\"Ready to explore! 🔍\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4f40a1",
   "metadata": {},
   "source": [
    "## 1. Load and First Look at Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0037003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a realistic e-commerce dataset\n",
    "np.random.seed(42)\n",
    "n_customers = 1000\n",
    "\n",
    "# Generate customer data\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': range(1, n_customers + 1),\n",
    "    'age': np.random.normal(35, 12, n_customers).clip(18, 70).astype(int),\n",
    "    'city': np.random.choice(['Perth', 'Sydney', 'Melbourne', 'Brisbane', 'Adelaide'], \n",
    "                            n_customers, p=[0.15, 0.3, 0.25, 0.2, 0.1]),\n",
    "    'member_type': np.random.choice(['Basic', 'Premium', 'VIP'], \n",
    "                                   n_customers, p=[0.6, 0.3, 0.1]),\n",
    "    'signup_date': pd.date_range(end='2025-08-25', periods=n_customers).to_list()\n",
    "})\n",
    "\n",
    "# Generate order data\n",
    "n_orders = 5000\n",
    "orders = pd.DataFrame({\n",
    "    'order_id': range(1, n_orders + 1),\n",
    "    'customer_id': np.random.choice(customers['customer_id'], n_orders),\n",
    "    'order_date': pd.date_range(end='2025-08-25', periods=n_orders),\n",
    "    'amount': np.random.lognormal(4, 1, n_orders).clip(10, 1000),\n",
    "    'items': np.random.poisson(3, n_orders).clip(1, 20),\n",
    "    'category': np.random.choice(['Electronics', 'Clothing', 'Books', 'Home', 'Sports'],\n",
    "                                n_orders, p=[0.25, 0.3, 0.15, 0.2, 0.1])\n",
    "})\n",
    "\n",
    "# Merge for complete dataset\n",
    "df = pd.merge(orders, customers, on='customer_id')\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741c1c71",
   "metadata": {},
   "source": [
    "## 2. Statistical Summary & Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127850d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Numerical Summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Extended statistics\n",
    "print(\"\\nExtended Statistics for Amount:\")\n",
    "amount_stats = {\n",
    "    'Mean': df['amount'].mean(),\n",
    "    'Median': df['amount'].median(),\n",
    "    'Mode': df['amount'].mode()[0] if len(df['amount'].mode()) > 0 else None,\n",
    "    'Std Dev': df['amount'].std(),\n",
    "    'Variance': df['amount'].var(),\n",
    "    'Skewness': df['amount'].skew(),\n",
    "    'Kurtosis': df['amount'].kurtosis(),\n",
    "    'IQR': df['amount'].quantile(0.75) - df['amount'].quantile(0.25),\n",
    "    'CV': df['amount'].std() / df['amount'].mean()  # Coefficient of variation\n",
    "}\n",
    "\n",
    "for stat, value in amount_stats.items():\n",
    "    print(f\"{stat:15}: {value:.2f}\")\n",
    "\n",
    "# Categorical summaries\n",
    "print(\"\\nCategorical Variables:\")\n",
    "for col in ['city', 'member_type', 'category']:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5265e60a",
   "metadata": {},
   "source": [
    "**Exercise 1 — Distribution Detective (medium)**  \n",
    "Check if order amounts follow a normal distribution using multiple methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582c78e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f25302",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Visual check\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df['amount'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Distribution of Order Amounts')\n",
    "axes[0].set_xlabel('Amount')\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(df['amount'], dist=\"norm\", plot=axes[1])\n",
    "axes[1].set_title('Q-Q Plot')\n",
    "\n",
    "# Log-transformed\n",
    "axes[2].hist(np.log(df['amount']), bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[2].set_title('Distribution of Log(Amount)')\n",
    "axes[2].set_xlabel('Log(Amount)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical test\n",
    "statistic, p_value = stats.shapiro(df['amount'].sample(min(5000, len(df))))\n",
    "print(f\"Shapiro-Wilk test: statistic={statistic:.4f}, p-value={p_value:.4f}\")\n",
    "print(f\"Normal distribution: {'Rejected' if p_value < 0.05 else 'Not rejected'}\")\n",
    "\n",
    "# Log-normal test\n",
    "log_stat, log_p = stats.shapiro(np.log(df['amount']).sample(min(5000, len(df))))\n",
    "print(f\"\\nLog-normal test: statistic={log_stat:.4f}, p-value={log_p:.4f}\")\n",
    "print(f\"Log-normal distribution: {'Rejected' if log_p < 0.05 else 'Not rejected'}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd91af14",
   "metadata": {},
   "source": [
    "## 3. Correlation and Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93cc6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare numeric features\n",
    "numeric_df = df[['amount', 'items', 'age']].copy()\n",
    "\n",
    "# Calculate correlations\n",
    "corr_matrix = numeric_df.corr()\n",
    "print(\"Correlation Matrix:\")\n",
    "print(corr_matrix)\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Relationship between categorical and numerical\n",
    "print(\"\\nAverage order amount by member type:\")\n",
    "member_analysis = df.groupby('member_type').agg({\n",
    "    'amount': ['mean', 'median', 'std', 'count']\n",
    "}).round(2)\n",
    "print(member_analysis)\n",
    "\n",
    "# Statistical test for difference\n",
    "groups = [df[df['member_type'] == mt]['amount'].values \n",
    "          for mt in df['member_type'].unique()]\n",
    "f_stat, p_value = stats.f_oneway(*groups)\n",
    "print(f\"\\nANOVA test for member types: F={f_stat:.2f}, p={p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac1848",
   "metadata": {},
   "source": [
    "## 4. Outlier Detection and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf3b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple outlier detection methods\n",
    "def detect_outliers(data, column):\n",
    "    # IQR method\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    iqr_outliers = data[(data[column] < lower) | (data[column] > upper)]\n",
    "    \n",
    "    # Z-score method\n",
    "    z_scores = np.abs(stats.zscore(data[column]))\n",
    "    z_outliers = data[z_scores > 3]\n",
    "    \n",
    "    # Isolation Forest (preview of ML)\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "    outlier_labels = iso_forest.fit_predict(data[[column]])\n",
    "    iso_outliers = data[outlier_labels == -1]\n",
    "    \n",
    "    return {\n",
    "        'IQR': iqr_outliers,\n",
    "        'Z-score': z_outliers,\n",
    "        'Isolation Forest': iso_outliers\n",
    "    }\n",
    "\n",
    "outliers = detect_outliers(df, 'amount')\n",
    "\n",
    "print(\"Outlier Detection Results:\")\n",
    "for method, outlier_df in outliers.items():\n",
    "    print(f\"\\n{method}: {len(outlier_df)} outliers ({len(outlier_df)/len(df)*100:.1f}%)\")\n",
    "    if len(outlier_df) > 0:\n",
    "        print(f\"  Range: ${outlier_df['amount'].min():.2f} - ${outlier_df['amount'].max():.2f}\")\n",
    "\n",
    "# Visualize outliers\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Boxplot\n",
    "axes[0].boxplot([df['amount'], df['items']], labels=['Amount', 'Items'])\n",
    "axes[0].set_title('Boxplot - Outlier Visualization')\n",
    "axes[0].set_ylabel('Value')\n",
    "\n",
    "# Scatter with outliers highlighted\n",
    "axes[1].scatter(df['items'], df['amount'], alpha=0.5, label='Normal')\n",
    "axes[1].scatter(outliers['IQR']['items'], outliers['IQR']['amount'], \n",
    "               color='red', label='Outliers (IQR)', s=50)\n",
    "axes[1].set_xlabel('Items')\n",
    "axes[1].set_ylabel('Amount')\n",
    "axes[1].set_title('Outliers in Context')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c85c5e",
   "metadata": {},
   "source": [
    "## 5. Temporal Patterns and Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f39466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based analysis\n",
    "df['order_month'] = df['order_date'].dt.to_period('M')\n",
    "df['order_weekday'] = df['order_date'].dt.day_name()\n",
    "df['order_day'] = df['order_date'].dt.day\n",
    "\n",
    "# Monthly trends\n",
    "monthly_stats = df.groupby('order_month').agg({\n",
    "    'amount': ['sum', 'mean', 'count'],\n",
    "    'customer_id': 'nunique'\n",
    "}).round(2)\n",
    "monthly_stats.columns = ['Total_Revenue', 'Avg_Order', 'Order_Count', 'Unique_Customers']\n",
    "print(\"Monthly Performance:\")\n",
    "print(monthly_stats.tail())\n",
    "\n",
    "# Day of week patterns\n",
    "weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "weekday_stats = df.groupby('order_weekday')['amount'].agg(['mean', 'count'])\n",
    "weekday_stats = weekday_stats.reindex(weekday_order)\n",
    "print(\"\\nWeekday Patterns:\")\n",
    "print(weekday_stats)\n",
    "\n",
    "# Customer lifecycle\n",
    "customer_stats = df.groupby('customer_id').agg({\n",
    "    'amount': ['sum', 'mean', 'count'],\n",
    "    'order_date': ['min', 'max']\n",
    "})\n",
    "customer_stats.columns = ['Total_Spent', 'Avg_Order', 'Order_Count', 'First_Order', 'Last_Order']\n",
    "customer_stats['Days_Active'] = (customer_stats['Last_Order'] - customer_stats['First_Order']).dt.days\n",
    "customer_stats['CLV_Category'] = pd.qcut(customer_stats['Total_Spent'], \n",
    "                                          q=4, labels=['Low', 'Medium', 'High', 'VIP'])\n",
    "\n",
    "print(\"\\nCustomer Lifetime Value Distribution:\")\n",
    "print(customer_stats['CLV_Category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdbd6b9",
   "metadata": {},
   "source": [
    "**Exercise 2 — Cohort Analysis (hard)**  \n",
    "Create a cohort analysis showing customer retention by signup month.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54870cb",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Prepare cohort data\n",
    "df['signup_month'] = pd.to_datetime(customers['signup_date']).dt.to_period('M')\n",
    "df['order_month_dt'] = df['order_date'].dt.to_period('M')\n",
    "\n",
    "# Create cohort table\n",
    "cohort_data = df.groupby(['signup_month', 'order_month_dt']).agg({\n",
    "    'customer_id': 'nunique'\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate periods since signup\n",
    "cohort_data['period_number'] = (cohort_data['order_month_dt'] - \n",
    "                                cohort_data['signup_month']).apply(lambda x: x.n)\n",
    "\n",
    "# Pivot for cohort table\n",
    "cohort_pivot = cohort_data.pivot_table(\n",
    "    index='signup_month',\n",
    "    columns='period_number',\n",
    "    values='customer_id'\n",
    ")\n",
    "\n",
    "# Calculate retention rates\n",
    "cohort_size = cohort_pivot.iloc[:, 0]\n",
    "retention = cohort_pivot.divide(cohort_size, axis=0) * 100\n",
    "\n",
    "print(\"Cohort Retention Rates (%)\")\n",
    "print(retention.iloc[:5, :5].round(1))  # First 5 cohorts, first 5 periods\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(retention.iloc[:10, :10], annot=True, fmt='.0f', \n",
    "            cmap='YlOrRd', vmin=0, vmax=100)\n",
    "plt.title('Customer Retention Cohort Analysis')\n",
    "plt.xlabel('Periods Since Signup')\n",
    "plt.ylabel('Signup Cohort')\n",
    "plt.show()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93813ac",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering & Derived Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f70e311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create meaningful features\n",
    "df['revenue_per_item'] = df['amount'] / df['items']\n",
    "df['is_weekend'] = df['order_date'].dt.dayofweek.isin([5, 6])\n",
    "df['is_high_value'] = df['amount'] > df['amount'].quantile(0.75)\n",
    "\n",
    "# Customer segmentation features\n",
    "customer_features = df.groupby('customer_id').agg({\n",
    "    'amount': ['sum', 'mean', 'std'],\n",
    "    'items': 'mean',\n",
    "    'order_id': 'count',\n",
    "    'category': lambda x: x.mode()[0] if len(x.mode()) > 0 else 'Unknown'\n",
    "}).round(2)\n",
    "\n",
    "customer_features.columns = ['Total_Revenue', 'Avg_Order_Value', 'Order_Volatility',\n",
    "                             'Avg_Items', 'Order_Frequency', 'Favorite_Category']\n",
    "\n",
    "# RFM Analysis (Recency, Frequency, Monetary)\n",
    "current_date = df['order_date'].max()\n",
    "rfm = df.groupby('customer_id').agg({\n",
    "    'order_date': lambda x: (current_date - x.max()).days,  # Recency\n",
    "    'order_id': 'count',  # Frequency\n",
    "    'amount': 'sum'  # Monetary\n",
    "})\n",
    "rfm.columns = ['Recency', 'Frequency', 'Monetary']\n",
    "\n",
    "# Create RFM segments\n",
    "rfm['R_Score'] = pd.qcut(rfm['Recency'], q=4, labels=['4', '3', '2', '1'])\n",
    "rfm['F_Score'] = pd.qcut(rfm['Frequency'].rank(method='first'), q=4, labels=['1', '2', '3', '4'])\n",
    "rfm['M_Score'] = pd.qcut(rfm['Monetary'], q=4, labels=['1', '2', '3', '4'])\n",
    "rfm['RFM_Score'] = rfm['R_Score'].astype(str) + rfm['F_Score'].astype(str) + rfm['M_Score'].astype(str)\n",
    "\n",
    "print(\"RFM Segmentation Summary:\")\n",
    "print(rfm.head(10))\n",
    "\n",
    "# Top segments\n",
    "print(\"\\nTop Customer Segments:\")\n",
    "print(rfm['RFM_Score'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de98963a",
   "metadata": {},
   "source": [
    "## 7. Data Quality & Integrity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7b1b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data quality report\n",
    "def comprehensive_eda_report(df):\n",
    "    report = {}\n",
    "    \n",
    "    # Basic info\n",
    "    report['shape'] = df.shape\n",
    "    report['memory_usage'] = df.memory_usage(deep=True).sum() / 1024**2  # MB\n",
    "    \n",
    "    # Missing values\n",
    "    report['missing_values'] = df.isnull().sum().to_dict()\n",
    "    report['missing_percentage'] = (df.isnull().sum() / len(df) * 100).to_dict()\n",
    "    \n",
    "    # Duplicates\n",
    "    report['duplicate_rows'] = df.duplicated().sum()\n",
    "    \n",
    "    # Data types\n",
    "    report['dtypes'] = df.dtypes.value_counts().to_dict()\n",
    "    \n",
    "    # Unique values\n",
    "    report['unique_counts'] = df.nunique().to_dict()\n",
    "    \n",
    "    # Numerical statistics\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        report['numeric_summary'] = df[numeric_cols].describe().to_dict()\n",
    "    \n",
    "    # Categorical statistics\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    if len(cat_cols) > 0:\n",
    "        report['categorical_summary'] = {}\n",
    "        for col in cat_cols:\n",
    "            report['categorical_summary'][col] = {\n",
    "                'unique': df[col].nunique(),\n",
    "                'most_common': df[col].mode()[0] if len(df[col].mode()) > 0 else None,\n",
    "                'frequency': df[col].value_counts().iloc[0] if len(df[col].value_counts()) > 0 else 0\n",
    "            }\n",
    "    \n",
    "    return report\n",
    "\n",
    "eda_report = comprehensive_eda_report(df)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"COMPREHENSIVE EDA REPORT\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nDataset Shape: {eda_report['shape']}\")\n",
    "print(f\"Memory Usage: {eda_report['memory_usage']:.2f} MB\")\n",
    "print(f\"Duplicate Rows: {eda_report['duplicate_rows']}\")\n",
    "print(f\"\\nData Types Distribution:\")\n",
    "for dtype, count in eda_report['dtypes'].items():\n",
    "    print(f\"  {dtype}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cecfa82",
   "metadata": {},
   "source": [
    "## 8. Key Insights & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f88bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate actionable insights\n",
    "insights = []\n",
    "\n",
    "# Insight 1: Best performing segment\n",
    "best_segment = df.groupby('member_type')['amount'].mean().idxmax()\n",
    "best_value = df.groupby('member_type')['amount'].mean().max()\n",
    "insights.append(f\"1. {best_segment} members have highest avg order value: ${best_value:.2f}\")\n",
    "\n",
    "# Insight 2: Peak shopping day\n",
    "peak_day = weekday_stats['mean'].idxmax()\n",
    "insights.append(f\"2. {peak_day} has highest average order value\")\n",
    "\n",
    "# Insight 3: Customer concentration\n",
    "top_10_pct = customer_stats.nlargest(int(len(customer_stats) * 0.1), 'Total_Spent')\n",
    "revenue_concentration = top_10_pct['Total_Spent'].sum() / customer_stats['Total_Spent'].sum()\n",
    "insights.append(f\"3. Top 10% of customers generate {revenue_concentration:.1%} of revenue\")\n",
    "\n",
    "# Insight 4: Category performance\n",
    "category_performance = df.groupby('category')['amount'].agg(['mean', 'count', 'sum'])\n",
    "best_category = category_performance['mean'].idxmax()\n",
    "insights.append(f\"4. {best_category} has highest average order value\")\n",
    "\n",
    "# Insight 5: Growth trend\n",
    "first_month_revenue = monthly_stats.iloc[0]['Total_Revenue']\n",
    "last_month_revenue = monthly_stats.iloc[-1]['Total_Revenue']\n",
    "growth_rate = (last_month_revenue - first_month_revenue) / first_month_revenue\n",
    "insights.append(f\"5. Revenue grew {growth_rate:.1%} from first to last month\")\n",
    "\n",
    "print(\"📊 KEY INSIGHTS FROM EDA\")\n",
    "print(\"=\"*50)\n",
    "for insight in insights:\n",
    "    print(insight)\n",
    "\n",
    "print(\"\\n📈 RECOMMENDED ACTIONS\")\n",
    "print(\"=\"*50)\n",
    "recommendations = [\n",
    "    \"• Focus marketing efforts on converting Basic to Premium members\",\n",
    "    f\"• Increase inventory and promotions for {peak_day}s\",\n",
    "    \"• Implement VIP loyalty program for top 10% customers\",\n",
    "    f\"• Expand {best_category} product line\",\n",
    "    \"• Investigate and replicate factors driving growth\"\n",
    "]\n",
    "for rec in recommendations:\n",
    "    print(rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f834bfd",
   "metadata": {},
   "source": [
    "## 9. Mini-Challenges\n",
    "- **M1 (easy):** Find the customer with highest average order value\n",
    "- **M2 (medium):** Identify seasonal patterns in the data\n",
    "- **M3 (hard):** Build a customer scoring system based on multiple factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a2e9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn - try the challenges!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba28e65",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solutions</b></summary>\n",
    "\n",
    "```python\n",
    "# M1 - Top customer by average order\n",
    "customer_avg = df.groupby('customer_id')['amount'].mean().sort_values(ascending=False)\n",
    "top_customer = customer_avg.index[0]\n",
    "print(f\"Customer {top_customer} has highest avg order: ${customer_avg.iloc[0]:.2f}\")\n",
    "\n",
    "# M2 - Seasonal patterns\n",
    "df['month'] = df['order_date'].dt.month\n",
    "df['season'] = df['month'].apply(lambda x: \n",
    "    'Summer' if x in [12, 1, 2] else\n",
    "    'Autumn' if x in [3, 4, 5] else\n",
    "    'Winter' if x in [6, 7, 8] else 'Spring')\n",
    "\n",
    "seasonal_analysis = df.groupby('season')['amount'].agg(['mean', 'sum', 'count'])\n",
    "print(\"\\nSeasonal Patterns:\")\n",
    "print(seasonal_analysis)\n",
    "\n",
    "# M3 - Customer scoring\n",
    "def customer_score(row):\n",
    "    score = 0\n",
    "    # Recency (lower is better)\n",
    "    if row['Recency'] <= 30: score += 30\n",
    "    elif row['Recency'] <= 60: score += 20\n",
    "    elif row['Recency'] <= 90: score += 10\n",
    "    \n",
    "    # Frequency\n",
    "    if row['Frequency'] >= 10: score += 30\n",
    "    elif row['Frequency'] >= 5: score += 20\n",
    "    elif row['Frequency'] >= 2: score += 10\n",
    "    \n",
    "    # Monetary\n",
    "    if row['Monetary'] >= 1000: score += 40\n",
    "    elif row['Monetary'] >= 500: score += 25\n",
    "    elif row['Monetary'] >= 200: score += 15\n",
    "    \n",
    "    return score\n",
    "\n",
    "rfm['Customer_Score'] = rfm.apply(customer_score, axis=1)\n",
    "rfm['Customer_Grade'] = pd.cut(rfm['Customer_Score'], \n",
    "                               bins=[0, 30, 60, 90, 100],\n",
    "                               labels=['D', 'C', 'B', 'A'])\n",
    "\n",
    "print(\"\\nCustomer Scoring Distribution:\")\n",
    "print(rfm['Customer_Grade'].value_counts())\n",
    "print(\"\\nTop 5 Customers by Score:\")\n",
    "print(rfm.nlargest(5, 'Customer_Score')[['Customer_Score', 'Customer_Grade']])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4d4790",
   "metadata": {},
   "source": [
    "## Wrap-Up & Next Steps\n",
    "✅ You've completed a comprehensive EDA workflow  \n",
    "✅ You can identify patterns, outliers, and relationships  \n",
    "✅ You generated actionable business insights  \n",
    "✅ You're ready to visualize these findings beautifully  \n",
    "\n",
    "**Week 2 Preview:** Transform these insights into interactive dashboards and compelling visualizations!\n",
    "\n",
    "**Assignment:** Apply this EDA workflow to your own dataset and create a report with 5 key insights.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
