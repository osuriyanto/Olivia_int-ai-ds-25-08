{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** ¬∑ *Intermediate AI & Data Science*\n",
    "### Week 03 ¬∑ Notebook 02 ‚Äî From DataFrames to Databases: Mental Model Mapping\n",
    "**Instructor:** Amir Charkhi  |  **Goal:** Master dataframes to databases.\n",
    "\n",
    "> Format: theory ‚Üí implementation ‚Üí best practices ‚Üí real-world application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ The Big Picture\n",
    "\n",
    "You've mastered pandas. You're comfortable with DataFrames. Now we're adding SQL to your toolkit.\n",
    "\n",
    "**Why both?**\n",
    "- **Pandas**: In-memory, flexible, great for exploration\n",
    "- **SQL**: Scalable, persistent, great for production\n",
    "\n",
    "Think of them as complementary tools:\n",
    "- Use SQL to **extract and reduce** data from large sources\n",
    "- Use pandas to **explore and visualize** the reduced data\n",
    "- Use SQL to **productionize** your proven analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment ready!\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Custom SQL display function\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def show_sql(query):\n",
    "    \"\"\"Pretty print SQL queries\"\"\"\n",
    "    display(Markdown(f\"```sql\\n{query}\\n```\"))\n",
    "\n",
    "print(\"‚úÖ Environment ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Setting Up Our Data Laboratory\n",
    "\n",
    "We'll use the same retail dataset from Week 1, but now in both pandas AND SQL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Created 10,000 transactions\n",
      "üë• Created 1,500 customers\n",
      "üè∑Ô∏è Created 200 products\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>quantity</th>\n",
       "      <th>date</th>\n",
       "      <th>store_id</th>\n",
       "      <th>price</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1127</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "      <td>NYC</td>\n",
       "      <td>264.05</td>\n",
       "      <td>528.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1460</td>\n",
       "      <td>98</td>\n",
       "      <td>2</td>\n",
       "      <td>2024-01-01 00:15:00</td>\n",
       "      <td>CHI</td>\n",
       "      <td>338.38</td>\n",
       "      <td>676.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>861</td>\n",
       "      <td>154</td>\n",
       "      <td>2</td>\n",
       "      <td>2024-01-01 00:30:00</td>\n",
       "      <td>PHX</td>\n",
       "      <td>441.58</td>\n",
       "      <td>883.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1295</td>\n",
       "      <td>158</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-01-01 00:45:00</td>\n",
       "      <td>NYC</td>\n",
       "      <td>267.93</td>\n",
       "      <td>803.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1131</td>\n",
       "      <td>110</td>\n",
       "      <td>4</td>\n",
       "      <td>2024-01-01 01:00:00</td>\n",
       "      <td>HOU</td>\n",
       "      <td>187.62</td>\n",
       "      <td>750.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transaction_id  customer_id  product_id  quantity                date  \\\n",
       "0               1         1127          74         2 2024-01-01 00:00:00   \n",
       "1               2         1460          98         2 2024-01-01 00:15:00   \n",
       "2               3          861         154         2 2024-01-01 00:30:00   \n",
       "3               4         1295         158         3 2024-01-01 00:45:00   \n",
       "4               5         1131         110         4 2024-01-01 01:00:00   \n",
       "\n",
       "  store_id  price  revenue  \n",
       "0      NYC 264.05   528.10  \n",
       "1      CHI 338.38   676.76  \n",
       "2      PHX 441.58   883.16  \n",
       "3      NYC 267.93   803.79  \n",
       "4      HOU 187.62   750.48  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create sample retail data (same structure as Week 1)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate sample data\n",
    "n_transactions = 10000\n",
    "n_customers = 1500\n",
    "n_products = 200\n",
    "\n",
    "# Create transactions\n",
    "transactions = pd.DataFrame({\n",
    "    'transaction_id': range(1, n_transactions + 1),\n",
    "    'customer_id': np.random.randint(1, n_customers + 1, n_transactions),\n",
    "    'product_id': np.random.randint(1, n_products + 1, n_transactions),\n",
    "    'quantity': np.random.randint(1, 5, n_transactions),\n",
    "    'date': pd.date_range('2024-01-01', periods=n_transactions, freq='15min'),\n",
    "    'store_id': np.random.choice(['NYC', 'LA', 'CHI', 'HOU', 'PHX'], n_transactions)\n",
    "})\n",
    "\n",
    "# Create products\n",
    "categories = ['Electronics', 'Clothing', 'Food', 'Books', 'Sports']\n",
    "products = pd.DataFrame({\n",
    "    'product_id': range(1, n_products + 1),\n",
    "    'product_name': [f'Product_{i}' for i in range(1, n_products + 1)],\n",
    "    'category': np.random.choice(categories, n_products),\n",
    "    'price': np.round(np.random.uniform(10, 500, n_products), 2)\n",
    "})\n",
    "\n",
    "# Create customers\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': range(1, n_customers + 1),\n",
    "    'customer_name': [f'Customer_{i}' for i in range(1, n_customers + 1)],\n",
    "    'city': np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'], n_customers),\n",
    "    'signup_date': pd.date_range('2023-01-01', periods=n_customers, freq='6H')\n",
    "})\n",
    "\n",
    "# Add revenue column\n",
    "transactions = transactions.merge(products[['product_id', 'price']], on='product_id')\n",
    "transactions['revenue'] = transactions['quantity'] * transactions['price']\n",
    "\n",
    "print(f\"üì¶ Created {len(transactions):,} transactions\")\n",
    "print(f\"üë• Created {len(customers):,} customers\")\n",
    "print(f\"üè∑Ô∏è Created {len(products):,} products\")\n",
    "\n",
    "# Preview the data\n",
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Database created and data loaded!\n"
     ]
    }
   ],
   "source": [
    "# Create SQLite database and load our data\n",
    "conn = sqlite3.connect('retail.db')\n",
    "\n",
    "# Load data into SQL\n",
    "transactions.to_sql('transactions', conn, if_exists='replace', index=False)\n",
    "products.to_sql('products', conn, if_exists='replace', index=False)\n",
    "customers.to_sql('customers', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Create a SQLAlchemy engine for pandas integration\n",
    "engine = create_engine('sqlite:///retail.db')\n",
    "\n",
    "print(\"‚úÖ Database created and data loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Tables in database:\n",
      "  - transactions: 10,000 rows\n",
      "  - products: 200 rows\n",
      "  - customers: 1,500 rows\n"
     ]
    }
   ],
   "source": [
    "# Verify tables\n",
    "tables = pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table'\", conn)\n",
    "print(\"\\nüìä Tables in database:\")\n",
    "for table in tables['name']:\n",
    "    count = pd.read_sql(f\"SELECT COUNT(*) as count FROM {table}\", conn).iloc[0, 0]\n",
    "    print(f\"  - {table}: {count:,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Part 1: Basic Operations - SELECT, WHERE, ORDER BY\n",
    "\n",
    "Let's start with the fundamentals. Every pandas operation has a SQL equivalent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Selecting Columns\n",
    "\n",
    "The most basic operation - choosing which columns to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêº Pandas approach:\n",
      "   transaction_id  customer_id  revenue\n",
      "0               1         1127   528.10\n",
      "1               2         1460   676.76\n",
      "2               3          861   883.16\n",
      "3               4         1295   803.79\n",
      "4               5         1131   750.48\n",
      "\n",
      "==================================================\n",
      "\n",
      "üóÑÔ∏è SQL approach:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "\n",
       "SELECT transaction_id, customer_id, revenue\n",
       "FROM transactions\n",
       "LIMIT 5\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   transaction_id  customer_id  revenue\n",
      "0               1         1127   528.10\n",
      "1               2         1460   676.76\n",
      "2               3          861   883.16\n",
      "3               4         1295   803.79\n",
      "4               5         1131   750.48\n",
      "\n",
      "‚úÖ Results are identical!\n"
     ]
    }
   ],
   "source": [
    "# PANDAS: Select specific columns\n",
    "pandas_result = transactions[['transaction_id', 'customer_id', 'revenue']].head()\n",
    "print(\"üêº Pandas approach:\")\n",
    "print(pandas_result)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: Select specific columns\n",
    "sql_query = \"\"\"\n",
    "SELECT transaction_id, customer_id, revenue\n",
    "FROM transactions\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL approach:\")\n",
    "show_sql(sql_query)\n",
    "sql_result = pd.read_sql(sql_query, conn)\n",
    "print(sql_result)\n",
    "\n",
    "print(\"\\n‚úÖ Results are identical!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Filtering Rows (WHERE clause)\n",
    "\n",
    "Filtering is where SQL starts to shine with complex conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêº Pandas filtering:\n",
      "df[(df['revenue'] > 500) & (df['store_id'] == 'NYC')]\n",
      "    transaction_id  revenue store_id\n",
      "0                1   528.10      NYC\n",
      "3                4   803.79      NYC\n",
      "7                8   777.34      NYC\n",
      "25              26  1199.04      NYC\n",
      "30              31  1324.74      NYC\n",
      "\n",
      "==================================================\n",
      "\n",
      "üóÑÔ∏è SQL filtering:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "\n",
       "SELECT transaction_id, revenue, store_id\n",
       "FROM transactions\n",
       "WHERE revenue > 500 \n",
       "  AND store_id = 'NYC'\n",
       "LIMIT 5\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   transaction_id  revenue store_id\n",
      "0               1   528.10      NYC\n",
      "1               4   803.79      NYC\n",
      "2               8   777.34      NYC\n",
      "3              26  1199.04      NYC\n",
      "4              31  1324.74      NYC\n",
      "\n",
      "üí° Pro Tip: SQL WHERE is often more readable for complex conditions!\n"
     ]
    }
   ],
   "source": [
    "# PANDAS: Multiple filter conditions\n",
    "pandas_filter = transactions[\n",
    "    (transactions['revenue'] > 500) & \n",
    "    (transactions['store_id'] == 'NYC')\n",
    "][['transaction_id', 'revenue', 'store_id']].head()\n",
    "\n",
    "print(\"üêº Pandas filtering:\")\n",
    "print(\"df[(df['revenue'] > 500) & (df['store_id'] == 'NYC')]\")\n",
    "print(pandas_filter)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: WHERE clause\n",
    "sql_query = \"\"\"\n",
    "SELECT transaction_id, revenue, store_id\n",
    "FROM transactions\n",
    "WHERE revenue > 500 \n",
    "  AND store_id = 'NYC'\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL filtering:\")\n",
    "show_sql(sql_query)\n",
    "sql_filter = pd.read_sql(sql_query, conn)\n",
    "print(sql_filter)\n",
    "\n",
    "# Pro tip comparison\n",
    "print(\"\\nüí° Pro Tip: SQL WHERE is often more readable for complex conditions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Sorting (ORDER BY)\n",
    "\n",
    "Sorting is fundamental for rankings and time series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêº Pandas sorting (top 10 by revenue):\n",
      "df.nlargest(10, 'revenue')\n",
      "      transaction_id  customer_id  revenue\n",
      "1268            1269          285  1933.76\n",
      "1876            1877          166  1933.76\n",
      "2045            2046         1187  1933.76\n",
      "2297            2298         1050  1933.76\n",
      "4599            4600          121  1933.76\n",
      "8809            8810          728  1933.76\n",
      "8836            8837           60  1933.76\n",
      "8894            8895         1158  1933.76\n",
      "9579            9580          466  1933.76\n",
      "9739            9740          376  1933.76\n",
      "\n",
      "==================================================\n",
      "\n",
      "üóÑÔ∏è SQL sorting:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "\n",
       "SELECT transaction_id, customer_id, revenue\n",
       "FROM transactions\n",
       "ORDER BY revenue DESC\n",
       "LIMIT 10\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   transaction_id  customer_id  revenue\n",
      "0            1269          285  1933.76\n",
      "1            1877          166  1933.76\n",
      "2            2046         1187  1933.76\n",
      "3            2298         1050  1933.76\n",
      "4            4600          121  1933.76\n",
      "5            8810          728  1933.76\n",
      "6            8837           60  1933.76\n",
      "7            8895         1158  1933.76\n",
      "8            9580          466  1933.76\n",
      "9            9740          376  1933.76\n"
     ]
    }
   ],
   "source": [
    "# PANDAS: Sort by multiple columns\n",
    "pandas_sorted = transactions.nlargest(10, 'revenue')[['transaction_id', 'customer_id', 'revenue']]\n",
    "\n",
    "print(\"üêº Pandas sorting (top 10 by revenue):\")\n",
    "print(\"df.nlargest(10, 'revenue')\")\n",
    "print(pandas_sorted)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: ORDER BY\n",
    "sql_query = \"\"\"\n",
    "SELECT transaction_id, customer_id, revenue\n",
    "FROM transactions\n",
    "ORDER BY revenue DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL sorting:\")\n",
    "show_sql(sql_query)\n",
    "sql_sorted = pd.read_sql(sql_query, conn)\n",
    "print(sql_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîó Part 2: Aggregations - GROUP BY\n",
    "\n",
    "This is where the mental models really start to connect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['transaction_id', 'customer_id', 'product_id', 'quantity', 'date',\n",
       "       'store_id', 'price', 'revenue'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Simple Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêº Pandas aggregation:\n",
      "df.groupby('store_id').agg({'revenue': ['sum', 'mean', 'count']})\n",
      "            revenue             \n",
      "                sum   mean count\n",
      "store_id                        \n",
      "CHI      1257251.81 622.09  2021\n",
      "HOU      1272553.55 628.73  2024\n",
      "LA       1194809.67 628.85  1900\n",
      "NYC      1193083.10 625.31  1908\n",
      "PHX      1311829.97 611.01  2147\n",
      "\n",
      "==================================================\n",
      "\n",
      "üóÑÔ∏è SQL aggregation:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "\n",
       "SELECT \n",
       "    store_id,\n",
       "    SUM(revenue) as revenue_sum,\n",
       "    AVG(revenue) as revenue_mean,\n",
       "    COUNT(*) as revenue_count\n",
       "FROM transactions\n",
       "GROUP BY store_id\n",
       "ORDER BY revenue_sum DESC\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  store_id  revenue_sum  revenue_mean  revenue_count\n",
      "0      PHX   1311829.97        611.01           2147\n",
      "1      HOU   1272553.55        628.73           2024\n",
      "2      CHI   1257251.81        622.09           2021\n",
      "3       LA   1194809.67        628.85           1900\n",
      "4      NYC   1193083.10        625.31           1908\n"
     ]
    }
   ],
   "source": [
    "# PANDAS: Group by store and calculate metrics\n",
    "pandas_agg = transactions.groupby('store_id').agg({\n",
    "    'revenue': ['sum', 'mean', 'count']\n",
    "}).round(2)\n",
    "\n",
    "print(\"üêº Pandas aggregation:\")\n",
    "print(\"df.groupby('store_id').agg({'revenue': ['sum', 'mean', 'count']})\")\n",
    "print(pandas_agg)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: GROUP BY with multiple aggregations\n",
    "sql_query = \"\"\"\n",
    "SELECT \n",
    "    store_id,\n",
    "    SUM(revenue) as revenue_sum,\n",
    "    AVG(revenue) as revenue_mean,\n",
    "    COUNT(*) as revenue_count\n",
    "FROM transactions\n",
    "GROUP BY store_id\n",
    "ORDER BY revenue_sum DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL aggregation:\")\n",
    "show_sql(sql_query)\n",
    "sql_agg = pd.read_sql(sql_query, conn)\n",
    "print(sql_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Multiple Grouping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêº Pandas multi-level groupby:\n",
      "df.groupby(['store_id', 'date_only'])['revenue'].sum()\n",
      "store_id  date_only \n",
      "CHI       2024-01-01   15293.12\n",
      "          2024-01-02   11716.84\n",
      "          2024-01-03   10267.87\n",
      "          2024-01-04    8972.93\n",
      "          2024-01-05   11157.58\n",
      "          2024-01-06   15685.74\n",
      "          2024-01-07   18873.03\n",
      "          2024-01-08   11635.85\n",
      "          2024-01-09    7796.28\n",
      "          2024-01-10   14762.60\n",
      "Name: revenue, dtype: float64\n",
      "\n",
      "==================================================\n",
      "\n",
      "üóÑÔ∏è SQL multi-level groupby:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "\n",
       "SELECT \n",
       "    store_id,\n",
       "    date_only,\n",
       "    SUM(revenue) as total_revenue\n",
       "FROM transactions\n",
       "GROUP BY store_id, date_only\n",
       "ORDER BY store_id, date_only\n",
       "LIMIT 10\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  store_id   date_only  total_revenue\n",
      "0      CHI  2024-01-01       15293.12\n",
      "1      CHI  2024-01-02       11716.84\n",
      "2      CHI  2024-01-03       10267.87\n",
      "3      CHI  2024-01-04        8972.93\n",
      "4      CHI  2024-01-05       11157.58\n",
      "5      CHI  2024-01-06       15685.74\n",
      "6      CHI  2024-01-07       18873.03\n",
      "7      CHI  2024-01-08       11635.85\n",
      "8      CHI  2024-01-09        7796.28\n",
      "9      CHI  2024-01-10       14762.60\n"
     ]
    }
   ],
   "source": [
    "# Add date components for better grouping\n",
    "transactions['date_only'] = transactions['date'].dt.date\n",
    "transactions['hour'] = transactions['date'].dt.hour\n",
    "\n",
    "# Update SQL table\n",
    "transactions.to_sql('transactions', conn, if_exists='replace', index=False)\n",
    "\n",
    "# PANDAS: Multi-level groupby\n",
    "pandas_multi = transactions.groupby(['store_id', 'date_only'])['revenue'].sum().head(10)\n",
    "\n",
    "print(\"üêº Pandas multi-level groupby:\")\n",
    "print(\"df.groupby(['store_id', 'date_only'])['revenue'].sum()\")\n",
    "print(pandas_multi)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: Multiple GROUP BY columns\n",
    "sql_query = \"\"\"\n",
    "SELECT \n",
    "    store_id,\n",
    "    date_only,\n",
    "    SUM(revenue) as total_revenue\n",
    "FROM transactions\n",
    "GROUP BY store_id, date_only\n",
    "ORDER BY store_id, date_only\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL multi-level groupby:\")\n",
    "show_sql(sql_query)\n",
    "sql_multi = pd.read_sql(sql_query, conn)\n",
    "print(sql_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Filtering After Aggregation (HAVING clause)\n",
    "\n",
    "This is a key concept - filtering AFTER grouping!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêº Pandas approach (filter after groupby):\n",
      "grouped = df.groupby('store_id')['revenue'].sum()\n",
      "grouped[grouped > 100000]\n",
      "store_id\n",
      "CHI   1257251.81\n",
      "HOU   1272553.55\n",
      "LA    1194809.67\n",
      "NYC   1193083.10\n",
      "PHX   1311829.97\n",
      "Name: revenue, dtype: float64\n",
      "\n",
      "==================================================\n",
      "\n",
      "üóÑÔ∏è SQL approach (HAVING clause):\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "\n",
       "SELECT \n",
       "    store_id,\n",
       "    SUM(revenue) as total_revenue\n",
       "FROM transactions\n",
       "GROUP BY store_id\n",
       "HAVING SUM(revenue) > 100000\n",
       "ORDER BY total_revenue DESC\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  store_id  total_revenue\n",
      "0      PHX     1311829.97\n",
      "1      HOU     1272553.55\n",
      "2      CHI     1257251.81\n",
      "3       LA     1194809.67\n",
      "4      NYC     1193083.10\n",
      "\n",
      "üí° Key Insight: WHERE filters rows BEFORE grouping, HAVING filters AFTER grouping!\n"
     ]
    }
   ],
   "source": [
    "# PANDAS: Filter after groupby\n",
    "store_totals = transactions.groupby('store_id')['revenue'].sum()\n",
    "pandas_having = store_totals[store_totals > 100000]\n",
    "\n",
    "print(\"üêº Pandas approach (filter after groupby):\")\n",
    "print(\"grouped = df.groupby('store_id')['revenue'].sum()\")\n",
    "print(\"grouped[grouped > 100000]\")\n",
    "print(pandas_having)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: HAVING clause\n",
    "sql_query = \"\"\"\n",
    "SELECT \n",
    "    store_id,\n",
    "    SUM(revenue) as total_revenue\n",
    "FROM transactions\n",
    "GROUP BY store_id\n",
    "HAVING SUM(revenue) > 100000\n",
    "ORDER BY total_revenue DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL approach (HAVING clause):\")\n",
    "show_sql(sql_query)\n",
    "sql_having = pd.read_sql(sql_query, conn)\n",
    "print(sql_having)\n",
    "\n",
    "print(\"\\nüí° Key Insight: WHERE filters rows BEFORE grouping, HAVING filters AFTER grouping!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîó Part 3: JOINs - Combining Tables\n",
    "\n",
    "JOINs are SQL's superpower. Let's map them to pandas merge operations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Inner Join (Default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêº Pandas inner join:\n",
      "df1.merge(df2, on='product_id', how='inner')\n",
      "   transaction_id  customer_id  product_id  revenue product_name category\n",
      "0               1         1127          74   528.10   Product_74     Food\n",
      "1               2         1460          98   676.76   Product_98    Books\n",
      "2               3          861         154   883.16  Product_154     Food\n",
      "3               4         1295         158   803.79  Product_158   Sports\n",
      "4               5         1131         110   750.48  Product_110   Sports\n",
      "\n",
      "==================================================\n",
      "\n",
      "üóÑÔ∏è SQL inner join:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "\n",
       "SELECT \n",
       "    t.transaction_id,\n",
       "    t.customer_id,\n",
       "    t.product_id,\n",
       "    t.revenue,\n",
       "    p.product_name,\n",
       "    p.category\n",
       "FROM transactions t\n",
       "INNER JOIN products p ON t.product_id = p.product_id\n",
       "LIMIT 5\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   transaction_id  customer_id  product_id  revenue product_name category\n",
      "0               1         1127          74   528.10   Product_74     Food\n",
      "1               2         1460          98   676.76   Product_98    Books\n",
      "2               3          861         154   883.16  Product_154     Food\n",
      "3               4         1295         158   803.79  Product_158   Sports\n",
      "4               5         1131         110   750.48  Product_110   Sports\n"
     ]
    }
   ],
   "source": [
    "# PANDAS: Inner join\n",
    "pandas_inner = transactions[['transaction_id', 'customer_id', 'product_id', 'revenue']].merge(\n",
    "    products[['product_id', 'product_name', 'category']],\n",
    "    on='product_id',\n",
    "    how='inner'\n",
    ").head()\n",
    "\n",
    "print(\"üêº Pandas inner join:\")\n",
    "print(\"df1.merge(df2, on='product_id', how='inner')\")\n",
    "print(pandas_inner)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: INNER JOIN\n",
    "sql_query = \"\"\"\n",
    "SELECT \n",
    "    t.transaction_id,\n",
    "    t.customer_id,\n",
    "    t.product_id,\n",
    "    t.revenue,\n",
    "    p.product_name,\n",
    "    p.category\n",
    "FROM transactions t\n",
    "INNER JOIN products p ON t.product_id = p.product_id\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL inner join:\")\n",
    "show_sql(sql_query)\n",
    "sql_inner = pd.read_sql(sql_query, conn)\n",
    "print(sql_inner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Left Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêº Pandas left join (all customers, even without purchases):\n",
      "customers.merge(revenue, on='customer_id', how='left').fillna(0)\n",
      "   customer_id customer_name  revenue\n",
      "0            1    Customer_1  5176.93\n",
      "1            2    Customer_2  2232.51\n",
      "2            3    Customer_3  1569.18\n",
      "3            4    Customer_4  2479.04\n",
      "4            5    Customer_5  3616.14\n",
      "5            6    Customer_6  3528.33\n",
      "6            7    Customer_7  4061.29\n",
      "7            8    Customer_8  2230.96\n",
      "8            9    Customer_9  2309.34\n",
      "9           10   Customer_10  5605.37\n",
      "\n",
      "==================================================\n",
      "\n",
      "üóÑÔ∏è SQL left join:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "\n",
       "SELECT \n",
       "    c.customer_id,\n",
       "    c.customer_name,\n",
       "    COALESCE(SUM(t.revenue), 0) as total_revenue\n",
       "FROM customers c\n",
       "LEFT JOIN transactions t ON c.customer_id = t.customer_id\n",
       "WHERE c.customer_id <= 10\n",
       "GROUP BY c.customer_id, c.customer_name\n",
       "ORDER BY c.customer_id\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   customer_id customer_name  total_revenue\n",
      "0            1    Customer_1        5176.93\n",
      "1            2    Customer_2        2232.51\n",
      "2            3    Customer_3        1569.18\n",
      "3            4    Customer_4        2479.04\n",
      "4            5    Customer_5        3616.14\n",
      "5            6    Customer_6        3528.33\n",
      "6            7    Customer_7        4061.29\n",
      "7            8    Customer_8        2230.96\n",
      "8            9    Customer_9        2309.34\n",
      "9           10   Customer_10        5605.37\n",
      "\n",
      "üí° COALESCE in SQL = fillna in pandas!\n"
     ]
    }
   ],
   "source": [
    "# Create some customers without transactions for demonstration\n",
    "all_customers = customers[['customer_id', 'customer_name']].head(10)\n",
    "customer_revenue = transactions.groupby('customer_id')['revenue'].sum().reset_index()\n",
    "\n",
    "# PANDAS: Left join\n",
    "pandas_left = all_customers.merge(\n",
    "    customer_revenue,\n",
    "    on='customer_id',\n",
    "    how='left'\n",
    ").fillna(0)\n",
    "\n",
    "print(\"üêº Pandas left join (all customers, even without purchases):\")\n",
    "print(\"customers.merge(revenue, on='customer_id', how='left').fillna(0)\")\n",
    "print(pandas_left)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: LEFT JOIN\n",
    "sql_query = \"\"\"\n",
    "SELECT \n",
    "    c.customer_id,\n",
    "    c.customer_name,\n",
    "    COALESCE(SUM(t.revenue), 0) as total_revenue\n",
    "FROM customers c\n",
    "LEFT JOIN transactions t ON c.customer_id = t.customer_id\n",
    "WHERE c.customer_id <= 10\n",
    "GROUP BY c.customer_id, c.customer_name\n",
    "ORDER BY c.customer_id\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL left join:\")\n",
    "show_sql(sql_query)\n",
    "sql_left = pd.read_sql(sql_query, conn)\n",
    "print(sql_left)\n",
    "\n",
    "print(\"\\nüí° COALESCE in SQL = fillna in pandas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Multiple Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêº Pandas multiple joins:\n",
      "df.merge(products, on='product_id').merge(customers, on='customer_id')\n",
      "   transaction_id  customer_id  product_id  revenue product_name category  \\\n",
      "0               1         1127          74   528.10   Product_74     Food   \n",
      "1               2         1460          98   676.76   Product_98    Books   \n",
      "2               3          861         154   883.16  Product_154     Food   \n",
      "3               4         1295         158   803.79  Product_158   Sports   \n",
      "4               5         1131         110   750.48  Product_110   Sports   \n",
      "\n",
      "   customer_name         city  \n",
      "0  Customer_1127     New York  \n",
      "1  Customer_1460  Los Angeles  \n",
      "2   Customer_861      Chicago  \n",
      "3  Customer_1295      Houston  \n",
      "4  Customer_1131      Phoenix  \n",
      "\n",
      "==================================================\n",
      "\n",
      "üóÑÔ∏è SQL multiple joins:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "\n",
       "SELECT \n",
       "    t.transaction_id,\n",
       "    c.customer_name,\n",
       "    c.city,\n",
       "    p.product_name,\n",
       "    p.category,\n",
       "    t.revenue\n",
       "FROM transactions t\n",
       "JOIN products p ON t.product_id = p.product_id\n",
       "JOIN customers c ON t.customer_id = c.customer_id\n",
       "LIMIT 5\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   transaction_id  customer_name         city product_name category  revenue\n",
      "0               1  Customer_1127     New York   Product_74     Food   528.10\n",
      "1               2  Customer_1460  Los Angeles   Product_98    Books   676.76\n",
      "2               3   Customer_861      Chicago  Product_154     Food   883.16\n",
      "3               4  Customer_1295      Houston  Product_158   Sports   803.79\n",
      "4               5  Customer_1131      Phoenix  Product_110   Sports   750.48\n"
     ]
    }
   ],
   "source": [
    "# PANDAS: Chain multiple merges\n",
    "pandas_multi_join = transactions[['transaction_id', 'customer_id', 'product_id', 'revenue']].merge(\n",
    "    products[['product_id', 'product_name', 'category']],\n",
    "    on='product_id'\n",
    ").merge(\n",
    "    customers[['customer_id', 'customer_name', 'city']],\n",
    "    on='customer_id'\n",
    ").head()\n",
    "\n",
    "print(\"üêº Pandas multiple joins:\")\n",
    "print(\"df.merge(products, on='product_id').merge(customers, on='customer_id')\")\n",
    "print(pandas_multi_join)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: Multiple JOINs\n",
    "sql_query = \"\"\"\n",
    "SELECT \n",
    "    t.transaction_id,\n",
    "    c.customer_name,\n",
    "    c.city,\n",
    "    p.product_name,\n",
    "    p.category,\n",
    "    t.revenue\n",
    "FROM transactions t\n",
    "JOIN products p ON t.product_id = p.product_id\n",
    "JOIN customers c ON t.customer_id = c.customer_id\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL multiple joins:\")\n",
    "show_sql(sql_query)\n",
    "sql_multi_join = pd.read_sql(sql_query, conn)\n",
    "print(sql_multi_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Part 4: Advanced Operations - Window Functions\n",
    "\n",
    "Window functions are incredibly powerful for analytics. Let's see how pandas and SQL compare!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Ranking Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêº Pandas ranking (top 3 transactions per store):\n",
      "df['rank'] = df.groupby('store_id')['revenue'].rank(method='dense', ascending=False)\n",
      "     store_id  transaction_id  revenue  rank_in_store\n",
      "1268      CHI            1269  1933.76           1.00\n",
      "9739      CHI            9740  1933.76           1.00\n",
      "2227      CHI            2228  1932.04           2.00\n",
      "2403      CHI            2404  1932.04           2.00\n",
      "5631      CHI            5632  1932.04           2.00\n",
      "1512      CHI            1513  1928.48           3.00\n",
      "3703      CHI            3704  1928.48           3.00\n",
      "6042      CHI            6043  1928.48           3.00\n",
      "7441      CHI            7442  1928.48           3.00\n",
      "8019      CHI            8020  1928.48           3.00\n",
      "\n",
      "==================================================\n",
      "\n",
      "üóÑÔ∏è SQL window function:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "\n",
       "WITH ranked_transactions AS (\n",
       "    SELECT \n",
       "        store_id,\n",
       "        transaction_id,\n",
       "        revenue,\n",
       "        DENSE_RANK() OVER (PARTITION BY store_id ORDER BY revenue DESC) as rank_in_store\n",
       "    FROM transactions\n",
       ")\n",
       "SELECT *\n",
       "FROM ranked_transactions\n",
       "WHERE rank_in_store <= 3\n",
       "ORDER BY store_id, rank_in_store\n",
       "LIMIT 10\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  store_id  transaction_id  revenue  rank_in_store\n",
      "0      CHI            1269  1933.76              1\n",
      "1      CHI            9740  1933.76              1\n",
      "2      CHI            2228  1932.04              2\n",
      "3      CHI            2404  1932.04              2\n",
      "4      CHI            5632  1932.04              2\n",
      "5      CHI            1513  1928.48              3\n",
      "6      CHI            3704  1928.48              3\n",
      "7      CHI            6043  1928.48              3\n",
      "8      CHI            7442  1928.48              3\n",
      "9      CHI            8020  1928.48              3\n"
     ]
    }
   ],
   "source": [
    "# PANDAS: Ranking within groups\n",
    "pandas_rank = transactions.copy()\n",
    "pandas_rank['rank_in_store'] = pandas_rank.groupby('store_id')['revenue'].rank(method='dense', ascending=False)\n",
    "top_per_store = pandas_rank[pandas_rank['rank_in_store'] <= 3][['store_id', 'transaction_id', 'revenue', 'rank_in_store']].sort_values(['store_id', 'rank_in_store'])\n",
    "\n",
    "print(\"üêº Pandas ranking (top 3 transactions per store):\")\n",
    "print(\"df['rank'] = df.groupby('store_id')['revenue'].rank(method='dense', ascending=False)\")\n",
    "print(top_per_store.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: Window function with RANK()\n",
    "sql_query = \"\"\"\n",
    "WITH ranked_transactions AS (\n",
    "    SELECT \n",
    "        store_id,\n",
    "        transaction_id,\n",
    "        revenue,\n",
    "        DENSE_RANK() OVER (PARTITION BY store_id ORDER BY revenue DESC) as rank_in_store\n",
    "    FROM transactions\n",
    ")\n",
    "SELECT *\n",
    "FROM ranked_transactions\n",
    "WHERE rank_in_store <= 3\n",
    "ORDER BY store_id, rank_in_store\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL window function:\")\n",
    "show_sql(sql_query)\n",
    "sql_rank = pd.read_sql(sql_query, conn)\n",
    "print(sql_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Running Totals and Moving Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêº Pandas cumulative and rolling:\n",
      "df['cumsum'] = df['revenue'].cumsum()\n",
      "df['rolling_avg'] = df['revenue'].rolling(window=7).mean()\n",
      "    date_only  revenue  cumulative_revenue  moving_avg_7d\n",
      "0  2024-01-01 59709.54            59709.54       59709.54\n",
      "1  2024-01-02 59637.14           119346.68       59673.34\n",
      "2  2024-01-03 52733.77           172080.45       57360.15\n",
      "3  2024-01-04 62495.57           234576.02       58644.01\n",
      "4  2024-01-05 57646.39           292222.41       58444.48\n",
      "5  2024-01-06 61774.59           353997.00       58999.50\n",
      "6  2024-01-07 62692.86           416689.86       59527.12\n",
      "7  2024-01-08 63542.02           480231.88       60074.62\n",
      "8  2024-01-09 59077.97           539309.85       59994.74\n",
      "9  2024-01-10 57337.61           596647.46       60652.43\n",
      "\n",
      "==================================================\n",
      "\n",
      "üóÑÔ∏è SQL window functions:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "\n",
       "WITH daily_totals AS (\n",
       "    SELECT \n",
       "        date_only,\n",
       "        SUM(revenue) as daily_revenue\n",
       "    FROM transactions\n",
       "    GROUP BY date_only\n",
       ")\n",
       "SELECT \n",
       "    date_only,\n",
       "    daily_revenue,\n",
       "    SUM(daily_revenue) OVER (ORDER BY date_only) as cumulative_revenue,\n",
       "    AVG(daily_revenue) OVER (\n",
       "        ORDER BY date_only \n",
       "        ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n",
       "    ) as moving_avg_7d\n",
       "FROM daily_totals\n",
       "ORDER BY date_only\n",
       "LIMIT 10\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    date_only  daily_revenue  cumulative_revenue  moving_avg_7d\n",
      "0  2024-01-01       59709.54            59709.54       59709.54\n",
      "1  2024-01-02       59637.14           119346.68       59673.34\n",
      "2  2024-01-03       52733.77           172080.45       57360.15\n",
      "3  2024-01-04       62495.57           234576.02       58644.01\n",
      "4  2024-01-05       57646.39           292222.41       58444.48\n",
      "5  2024-01-06       61774.59           353997.00       58999.50\n",
      "6  2024-01-07       62692.86           416689.86       59527.12\n",
      "7  2024-01-08       63542.02           480231.88       60074.62\n",
      "8  2024-01-09       59077.97           539309.85       59994.74\n",
      "9  2024-01-10       57337.61           596647.46       60652.43\n"
     ]
    }
   ],
   "source": [
    "# PANDAS: Cumulative sum and rolling average\n",
    "daily_revenue = transactions.groupby('date_only')['revenue'].sum().reset_index()\n",
    "daily_revenue = daily_revenue.sort_values('date_only')\n",
    "daily_revenue['cumulative_revenue'] = daily_revenue['revenue'].cumsum()\n",
    "daily_revenue['moving_avg_7d'] = daily_revenue['revenue'].rolling(window=7, min_periods=1).mean()\n",
    "\n",
    "print(\"üêº Pandas cumulative and rolling:\")\n",
    "print(\"df['cumsum'] = df['revenue'].cumsum()\")\n",
    "print(\"df['rolling_avg'] = df['revenue'].rolling(window=7).mean()\")\n",
    "print(daily_revenue.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: Window functions for running totals\n",
    "sql_query = \"\"\"\n",
    "WITH daily_totals AS (\n",
    "    SELECT \n",
    "        date_only,\n",
    "        SUM(revenue) as daily_revenue\n",
    "    FROM transactions\n",
    "    GROUP BY date_only\n",
    ")\n",
    "SELECT \n",
    "    date_only,\n",
    "    daily_revenue,\n",
    "    SUM(daily_revenue) OVER (ORDER BY date_only) as cumulative_revenue,\n",
    "    AVG(daily_revenue) OVER (\n",
    "        ORDER BY date_only \n",
    "        ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n",
    "    ) as moving_avg_7d\n",
    "FROM daily_totals\n",
    "ORDER BY date_only\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL window functions:\")\n",
    "show_sql(sql_query)\n",
    "sql_window = pd.read_sql(sql_query, conn)\n",
    "print(sql_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Lead and Lag Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêº Pandas shift operations:\n",
      "df['prev'] = df['revenue'].shift(1)\n",
      "df['next'] = df['revenue'].shift(-1)\n",
      "    date_only  revenue  prev_day_revenue  next_day_revenue  \\\n",
      "0  2024-01-01 59709.54               NaN          59637.14   \n",
      "1  2024-01-02 59637.14          59709.54          52733.77   \n",
      "2  2024-01-03 52733.77          59637.14          62495.57   \n",
      "3  2024-01-04 62495.57          52733.77          57646.39   \n",
      "4  2024-01-05 57646.39          62495.57          61774.59   \n",
      "5  2024-01-06 61774.59          57646.39          62692.86   \n",
      "6  2024-01-07 62692.86          61774.59          63542.02   \n",
      "7  2024-01-08 63542.02          62692.86          59077.97   \n",
      "8  2024-01-09 59077.97          63542.02          57337.61   \n",
      "9  2024-01-10 57337.61          59077.97          61057.75   \n",
      "\n",
      "   day_over_day_change  \n",
      "0                  NaN  \n",
      "1               -72.40  \n",
      "2             -6903.37  \n",
      "3              9761.80  \n",
      "4             -4849.18  \n",
      "5              4128.20  \n",
      "6               918.27  \n",
      "7               849.16  \n",
      "8             -4464.05  \n",
      "9             -1740.36  \n",
      "\n",
      "==================================================\n",
      "\n",
      "üóÑÔ∏è SQL LAG/LEAD:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "\n",
       "WITH daily_totals AS (\n",
       "    SELECT \n",
       "        date_only,\n",
       "        SUM(revenue) as daily_revenue\n",
       "    FROM transactions\n",
       "    GROUP BY date_only\n",
       ")\n",
       "SELECT \n",
       "    date_only,\n",
       "    daily_revenue,\n",
       "    LAG(daily_revenue, 1) OVER (ORDER BY date_only) as prev_day_revenue,\n",
       "    LEAD(daily_revenue, 1) OVER (ORDER BY date_only) as next_day_revenue,\n",
       "    daily_revenue - LAG(daily_revenue, 1) OVER (ORDER BY date_only) as day_over_day_change\n",
       "FROM daily_totals\n",
       "ORDER BY date_only\n",
       "LIMIT 10\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    date_only  daily_revenue  prev_day_revenue  next_day_revenue  \\\n",
      "0  2024-01-01       59709.54               NaN          59637.14   \n",
      "1  2024-01-02       59637.14          59709.54          52733.77   \n",
      "2  2024-01-03       52733.77          59637.14          62495.57   \n",
      "3  2024-01-04       62495.57          52733.77          57646.39   \n",
      "4  2024-01-05       57646.39          62495.57          61774.59   \n",
      "5  2024-01-06       61774.59          57646.39          62692.86   \n",
      "6  2024-01-07       62692.86          61774.59          63542.02   \n",
      "7  2024-01-08       63542.02          62692.86          59077.97   \n",
      "8  2024-01-09       59077.97          63542.02          57337.61   \n",
      "9  2024-01-10       57337.61          59077.97          61057.75   \n",
      "\n",
      "   day_over_day_change  \n",
      "0                  NaN  \n",
      "1               -72.40  \n",
      "2             -6903.37  \n",
      "3              9761.80  \n",
      "4             -4849.18  \n",
      "5              4128.20  \n",
      "6               918.27  \n",
      "7               849.16  \n",
      "8             -4464.05  \n",
      "9             -1740.36  \n"
     ]
    }
   ],
   "source": [
    "# PANDAS: Shift operations for time series\n",
    "daily_revenue = transactions.groupby('date_only')['revenue'].sum().reset_index().sort_values('date_only')\n",
    "daily_revenue['prev_day_revenue'] = daily_revenue['revenue'].shift(1)\n",
    "daily_revenue['next_day_revenue'] = daily_revenue['revenue'].shift(-1)\n",
    "daily_revenue['day_over_day_change'] = daily_revenue['revenue'] - daily_revenue['prev_day_revenue']\n",
    "\n",
    "print(\"üêº Pandas shift operations:\")\n",
    "print(\"df['prev'] = df['revenue'].shift(1)\")\n",
    "print(\"df['next'] = df['revenue'].shift(-1)\")\n",
    "print(daily_revenue.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: LAG and LEAD functions\n",
    "sql_query = \"\"\"\n",
    "WITH daily_totals AS (\n",
    "    SELECT \n",
    "        date_only,\n",
    "        SUM(revenue) as daily_revenue\n",
    "    FROM transactions\n",
    "    GROUP BY date_only\n",
    ")\n",
    "SELECT \n",
    "    date_only,\n",
    "    daily_revenue,\n",
    "    LAG(daily_revenue, 1) OVER (ORDER BY date_only) as prev_day_revenue,\n",
    "    LEAD(daily_revenue, 1) OVER (ORDER BY date_only) as next_day_revenue,\n",
    "    daily_revenue - LAG(daily_revenue, 1) OVER (ORDER BY date_only) as day_over_day_change\n",
    "FROM daily_totals\n",
    "ORDER BY date_only\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL LAG/LEAD:\")\n",
    "show_sql(sql_query)\n",
    "sql_lag_lead = pd.read_sql(sql_query, conn)\n",
    "print(sql_lag_lead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Part 5: Subqueries and CTEs\n",
    "\n",
    "Complex analytical questions often require multiple steps. Let's see how to structure them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Subqueries vs Method Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêº Pandas: Customers with avg order > $622.95\n",
      "     customer_id  avg_revenue\n",
      "318          319      1905.36\n",
      "296          297      1397.41\n",
      "120          121      1392.62\n",
      "967          968      1375.14\n",
      "842          843      1264.84\n",
      "635          636      1252.44\n",
      "43            44      1249.63\n",
      "97            98      1237.95\n",
      "417          418      1216.89\n",
      "167          168      1214.04\n",
      "\n",
      "==================================================\n",
      "\n",
      "üóÑÔ∏è SQL with subquery:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "\n",
       "SELECT \n",
       "    customer_id,\n",
       "    AVG(revenue) as avg_revenue\n",
       "FROM transactions\n",
       "GROUP BY customer_id\n",
       "HAVING AVG(revenue) > (\n",
       "    SELECT AVG(revenue) \n",
       "    FROM transactions\n",
       ")\n",
       "ORDER BY avg_revenue DESC\n",
       "LIMIT 10\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   customer_id  avg_revenue\n",
      "0          319      1905.36\n",
      "1          297      1397.41\n",
      "2          121      1392.62\n",
      "3          968      1375.14\n",
      "4          843      1264.84\n",
      "5          636      1252.44\n",
      "6           44      1249.63\n",
      "7           98      1237.95\n",
      "8          418      1216.89\n",
      "9          168      1214.04\n"
     ]
    }
   ],
   "source": [
    "# PANDAS: Method chaining for complex logic\n",
    "# Find customers whose average order is above the overall average\n",
    "overall_avg = transactions['revenue'].mean()\n",
    "\n",
    "pandas_complex = (\n",
    "    transactions\n",
    "    .groupby('customer_id')['revenue']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'revenue': 'avg_revenue'})\n",
    "    .query(f'avg_revenue > {overall_avg}')\n",
    "    .sort_values('avg_revenue', ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "print(f\"üêº Pandas: Customers with avg order > ${overall_avg:.2f}\")\n",
    "print(pandas_complex)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: Using subquery\n",
    "sql_query = \"\"\"\n",
    "SELECT \n",
    "    customer_id,\n",
    "    AVG(revenue) as avg_revenue\n",
    "FROM transactions\n",
    "GROUP BY customer_id\n",
    "HAVING AVG(revenue) > (\n",
    "    SELECT AVG(revenue) \n",
    "    FROM transactions\n",
    ")\n",
    "ORDER BY avg_revenue DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL with subquery:\")\n",
    "show_sql(sql_query)\n",
    "sql_subquery = pd.read_sql(sql_query, conn)\n",
    "print(sql_subquery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Common Table Expressions (CTEs)\n",
    "\n",
    "CTEs are like creating temporary DataFrames in your SQL query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêº Pandas multi-step analysis:\n",
      "                  customer_id  total_revenue\n",
      "customer_segment                            \n",
      "Low                        46         657.00\n",
      "Medium                    989        3223.83\n",
      "High                      463        6503.10\n",
      "\n",
      "==================================================\n",
      "\n",
      "üóÑÔ∏è SQL with CTEs:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "\n",
       "WITH customer_metrics AS (\n",
       "    SELECT \n",
       "        customer_id,\n",
       "        SUM(revenue) as total_revenue,\n",
       "        AVG(revenue) as avg_revenue,\n",
       "        COUNT(*) as transaction_count\n",
       "    FROM transactions\n",
       "    GROUP BY customer_id\n",
       "),\n",
       "customer_segments AS (\n",
       "    SELECT \n",
       "        *,\n",
       "        CASE \n",
       "            WHEN total_revenue <= 1000 THEN 'Low'\n",
       "            WHEN total_revenue <= 5000 THEN 'Medium'\n",
       "            ELSE 'High'\n",
       "        END as customer_segment\n",
       "    FROM customer_metrics\n",
       ")\n",
       "SELECT \n",
       "    customer_segment,\n",
       "    COUNT(*) as customer_count,\n",
       "    AVG(total_revenue) as avg_segment_revenue\n",
       "FROM customer_segments\n",
       "GROUP BY customer_segment\n",
       "ORDER BY avg_segment_revenue\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  customer_segment  customer_count  avg_segment_revenue\n",
      "0              Low              46               657.00\n",
      "1           Medium             989              3223.83\n",
      "2             High             463              6503.10\n",
      "\n",
      "üí° CTEs make complex SQL queries readable and modular, just like method chaining in pandas!\n"
     ]
    }
   ],
   "source": [
    "# PANDAS: Multi-step analysis\n",
    "# Step 1: Calculate customer metrics\n",
    "customer_metrics = transactions.groupby('customer_id').agg({\n",
    "    'revenue': ['sum', 'mean', 'count']\n",
    "}).round(2)\n",
    "customer_metrics.columns = ['total_revenue', 'avg_revenue', 'transaction_count']\n",
    "customer_metrics = customer_metrics.reset_index()\n",
    "\n",
    "# Step 2: Categorize customers\n",
    "customer_metrics['customer_segment'] = pd.cut(\n",
    "    customer_metrics['total_revenue'],\n",
    "    bins=[0, 1000, 5000, float('inf')],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "# Step 3: Summary by segment\n",
    "segment_summary = customer_metrics.groupby('customer_segment').agg({\n",
    "    'customer_id': 'count',\n",
    "    'total_revenue': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"üêº Pandas multi-step analysis:\")\n",
    "print(segment_summary)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# SQL: Using CTEs for the same analysis\n",
    "sql_query = \"\"\"\n",
    "WITH customer_metrics AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        SUM(revenue) as total_revenue,\n",
    "        AVG(revenue) as avg_revenue,\n",
    "        COUNT(*) as transaction_count\n",
    "    FROM transactions\n",
    "    GROUP BY customer_id\n",
    "),\n",
    "customer_segments AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        CASE \n",
    "            WHEN total_revenue <= 1000 THEN 'Low'\n",
    "            WHEN total_revenue <= 5000 THEN 'Medium'\n",
    "            ELSE 'High'\n",
    "        END as customer_segment\n",
    "    FROM customer_metrics\n",
    ")\n",
    "SELECT \n",
    "    customer_segment,\n",
    "    COUNT(*) as customer_count,\n",
    "    AVG(total_revenue) as avg_segment_revenue\n",
    "FROM customer_segments\n",
    "GROUP BY customer_segment\n",
    "ORDER BY avg_segment_revenue\n",
    "\"\"\"\n",
    "\n",
    "print(\"üóÑÔ∏è SQL with CTEs:\")\n",
    "show_sql(sql_query)\n",
    "sql_cte = pd.read_sql(sql_query, conn)\n",
    "print(sql_cte)\n",
    "\n",
    "print(\"\\nüí° CTEs make complex SQL queries readable and modular, just like method chaining in pandas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Part 6: Performance Considerations\n",
    "\n",
    "When should you use pandas vs SQL? Let's understand the trade-offs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Test 1: Simple filtering (revenue > 100)\n",
      "\n",
      "üêº Pandas: 9,340 rows in 0.0026 seconds\n",
      "üóÑÔ∏è SQL: 9,340 rows in 0.0313 seconds\n",
      "\n",
      "‚ö° Faster: Pandas by 0.0287s\n",
      "\n",
      "==================================================\n",
      "\n",
      "üìä Test 2: Complex aggregation (group by store, calculate multiple metrics)\n",
      "\n",
      "üêº Pandas: Aggregated in 0.0061 seconds\n",
      "üóÑÔ∏è SQL: Aggregated in 0.0050 seconds\n",
      "\n",
      "‚ö° Faster: SQL by 0.0010s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Test 1: Simple filtering\n",
    "print(\"üìä Test 1: Simple filtering (revenue > 100)\\n\")\n",
    "\n",
    "# Pandas timing\n",
    "start = time.time()\n",
    "pandas_result = transactions[transactions['revenue'] > 100]\n",
    "pandas_time = time.time() - start\n",
    "print(f\"üêº Pandas: {len(pandas_result):,} rows in {pandas_time:.4f} seconds\")\n",
    "\n",
    "# SQL timing\n",
    "start = time.time()\n",
    "sql_result = pd.read_sql(\"SELECT * FROM transactions WHERE revenue > 100\", conn)\n",
    "sql_time = time.time() - start\n",
    "print(f\"üóÑÔ∏è SQL: {len(sql_result):,} rows in {sql_time:.4f} seconds\")\n",
    "\n",
    "print(f\"\\n‚ö° Faster: {'Pandas' if pandas_time < sql_time else 'SQL'} by {abs(pandas_time - sql_time):.4f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Test 2: Complex aggregation\n",
    "print(\"üìä Test 2: Complex aggregation (group by store, calculate multiple metrics)\\n\")\n",
    "\n",
    "# Pandas timing\n",
    "start = time.time()\n",
    "pandas_agg = transactions.groupby('store_id').agg({\n",
    "    'revenue': ['sum', 'mean', 'std'],\n",
    "    'quantity': ['sum', 'mean'],\n",
    "    'transaction_id': 'count'\n",
    "})\n",
    "pandas_time = time.time() - start\n",
    "print(f\"üêº Pandas: Aggregated in {pandas_time:.4f} seconds\")\n",
    "\n",
    "# SQL timing\n",
    "start = time.time()\n",
    "sql_agg = pd.read_sql(\"\"\"\n",
    "    SELECT \n",
    "        store_id,\n",
    "        SUM(revenue) as revenue_sum,\n",
    "        AVG(revenue) as revenue_mean,\n",
    "        SUM(quantity) as quantity_sum,\n",
    "        AVG(quantity) as quantity_mean,\n",
    "        COUNT(*) as transaction_count\n",
    "    FROM transactions\n",
    "    GROUP BY store_id\n",
    "\"\"\", conn)\n",
    "sql_time = time.time() - start\n",
    "print(f\"üóÑÔ∏è SQL: Aggregated in {sql_time:.4f} seconds\")\n",
    "\n",
    "print(f\"\\n‚ö° Faster: {'Pandas' if pandas_time < sql_time else 'SQL'} by {abs(pandas_time - sql_time):.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä When to Use Each Tool\n",
    "\n",
    "Based on our experiments and real-world experience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ DECISION MATRIX: When to Use Pandas vs SQL\n",
      "\n",
      "üêº Pandas Data exploration & prototyping\n",
      "    ‚Üí Interactive, flexible, great for iteration\n",
      "\n",
      "üóÑÔ∏è SQL Production data pipelines\n",
      "    ‚Üí Scalable, auditable, version-controlled\n",
      "\n",
      "üêº Pandas Complex statistical analysis\n",
      "    ‚Üí Rich statistical libraries (scipy, statsmodels)\n",
      "\n",
      "üóÑÔ∏è SQL Data > 1GB\n",
      "    ‚Üí Memory constraints, let database do the work\n",
      "\n",
      "üóÑÔ∏è SQL Real-time dashboards\n",
      "    ‚Üí Direct queries, no data movement\n",
      "\n",
      "üóÑÔ∏è SQL Ad-hoc business queries\n",
      "    ‚Üí Standard language, shareable queries\n",
      "\n",
      "üêº Pandas ‚Üí SQL Machine learning features\n",
      "    ‚Üí Prototype in pandas, productionize in SQL\n",
      "\n",
      "üêº Pandas Data validation & cleaning\n",
      "    ‚Üí Better string/regex operations\n",
      "\n",
      "üêº Pandas Time series manipulation\n",
      "    ‚Üí Superior datetime handling\n",
      "\n",
      "üóÑÔ∏è SQL Joining multiple large tables\n",
      "    ‚Üí Optimized query planner\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decision_matrix = pd.DataFrame({\n",
    "    'Scenario': [\n",
    "        'Data exploration & prototyping',\n",
    "        'Production data pipelines',\n",
    "        'Complex statistical analysis',\n",
    "        'Data > 1GB',\n",
    "        'Real-time dashboards',\n",
    "        'Ad-hoc business queries',\n",
    "        'Machine learning features',\n",
    "        'Data validation & cleaning',\n",
    "        'Time series manipulation',\n",
    "        'Joining multiple large tables'\n",
    "    ],\n",
    "    'Preferred Tool': [\n",
    "        'üêº Pandas',\n",
    "        'üóÑÔ∏è SQL',\n",
    "        'üêº Pandas',\n",
    "        'üóÑÔ∏è SQL',\n",
    "        'üóÑÔ∏è SQL',\n",
    "        'üóÑÔ∏è SQL',\n",
    "        'üêº Pandas ‚Üí SQL',\n",
    "        'üêº Pandas',\n",
    "        'üêº Pandas',\n",
    "        'üóÑÔ∏è SQL'\n",
    "    ],\n",
    "    'Reason': [\n",
    "        'Interactive, flexible, great for iteration',\n",
    "        'Scalable, auditable, version-controlled',\n",
    "        'Rich statistical libraries (scipy, statsmodels)',\n",
    "        'Memory constraints, let database do the work',\n",
    "        'Direct queries, no data movement',\n",
    "        'Standard language, shareable queries',\n",
    "        'Prototype in pandas, productionize in SQL',\n",
    "        'Better string/regex operations',\n",
    "        'Superior datetime handling',\n",
    "        'Optimized query planner'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"üéØ DECISION MATRIX: When to Use Pandas vs SQL\\n\")\n",
    "for _, row in decision_matrix.iterrows():\n",
    "    print(f\"{row['Preferred Tool']} {row['Scenario']}\")\n",
    "    print(f\"    ‚Üí {row['Reason']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí° Part 7: Hybrid Workflows - Best of Both Worlds\n",
    "\n",
    "The real power comes from combining pandas and SQL seamlessly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Using SQL for Data Reduction, Pandas for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: SQL for heavy lifting\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "\n",
       "SELECT \n",
       "    date_only,\n",
       "    store_id,\n",
       "    COUNT(DISTINCT customer_id) as unique_customers,\n",
       "    SUM(revenue) as total_revenue,\n",
       "    AVG(revenue) as avg_transaction\n",
       "FROM transactions\n",
       "WHERE revenue > 50  -- Pre-filter in SQL\n",
       "GROUP BY date_only, store_id\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Reduced to 525 rows\n",
      "\n",
      "Step 2: Pandas for complex analysis\n",
      "   date_only store_id  unique_customers  total_revenue  avg_transaction  \\\n",
      "0 2024-01-01      CHI                23       15270.54           663.94   \n",
      "1 2024-01-01      HOU                17       13985.56           822.68   \n",
      "2 2024-01-01       LA                16        9338.46           583.65   \n",
      "3 2024-01-01      NYC                17       10206.44           600.38   \n",
      "4 2024-01-01      PHX                22       10885.96           494.82   \n",
      "5 2024-01-02      CHI                19       11619.60           611.56   \n",
      "6 2024-01-02      HOU                16        8424.43           526.53   \n",
      "7 2024-01-02       LA                16        9202.91           575.18   \n",
      "8 2024-01-02      NYC                18       11879.36           659.96   \n",
      "9 2024-01-02      PHX                23       18366.16           798.53   \n",
      "\n",
      "  day_of_week  is_weekend  daily_rank  \n",
      "0      Monday       False        1.00  \n",
      "1      Monday       False        2.00  \n",
      "2      Monday       False        5.00  \n",
      "3      Monday       False        4.00  \n",
      "4      Monday       False        3.00  \n",
      "5     Tuesday       False        3.00  \n",
      "6     Tuesday       False        5.00  \n",
      "7     Tuesday       False        4.00  \n",
      "8     Tuesday       False        2.00  \n",
      "9     Tuesday       False        1.00  \n",
      "\n",
      "üí° Best Practice: Use SQL to reduce data volume, pandas for complex transformations!\n"
     ]
    }
   ],
   "source": [
    "# Hybrid approach: Let SQL do the heavy lifting, pandas for fine-tuning\n",
    "\n",
    "# Step 1: Use SQL to filter and aggregate large data\n",
    "sql_query = \"\"\"\n",
    "SELECT \n",
    "    date_only,\n",
    "    store_id,\n",
    "    COUNT(DISTINCT customer_id) as unique_customers,\n",
    "    SUM(revenue) as total_revenue,\n",
    "    AVG(revenue) as avg_transaction\n",
    "FROM transactions\n",
    "WHERE revenue > 50  -- Pre-filter in SQL\n",
    "GROUP BY date_only, store_id\n",
    "\"\"\"\n",
    "\n",
    "print(\"Step 1: SQL for heavy lifting\")\n",
    "show_sql(sql_query)\n",
    "\n",
    "# Execute and get results\n",
    "daily_store_metrics = pd.read_sql(sql_query, conn)\n",
    "print(f\"\\n‚úÖ Reduced to {len(daily_store_metrics):,} rows\\n\")\n",
    "\n",
    "# Step 2: Use pandas for complex transformations\n",
    "print(\"Step 2: Pandas for complex analysis\")\n",
    "\n",
    "# Convert to datetime\n",
    "daily_store_metrics['date_only'] = pd.to_datetime(daily_store_metrics['date_only'])\n",
    "\n",
    "# Add time-based features\n",
    "daily_store_metrics['day_of_week'] = daily_store_metrics['date_only'].dt.day_name()\n",
    "daily_store_metrics['is_weekend'] = daily_store_metrics['date_only'].dt.dayofweek.isin([5, 6])\n",
    "\n",
    "# Calculate store performance ranking by day\n",
    "daily_store_metrics['daily_rank'] = daily_store_metrics.groupby('date_only')['total_revenue'].rank(ascending=False)\n",
    "\n",
    "# Show results\n",
    "print(daily_store_metrics.head(10))\n",
    "\n",
    "print(\"\\nüí° Best Practice: Use SQL to reduce data volume, pandas for complex transformations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Parameterized Queries from Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîí Safe parameterized query result:\n",
      "   transaction_id                 date product_name     category  revenue\n",
      "0            9776  2024-04-11 19:45:00  Product_178     Clothing   641.86\n",
      "1            7728  2024-03-21 11:45:00    Product_5       Sports   936.63\n",
      "2            2467  2024-01-26 16:30:00  Product_142         Food   601.92\n",
      "3            1469  2024-01-16 07:00:00   Product_71  Electronics  1163.91\n",
      "4            1397  2024-01-15 13:00:00  Product_137       Sports   245.34\n",
      "\n",
      "‚ö†Ô∏è NEVER use string formatting for SQL queries - always use parameters!\n"
     ]
    }
   ],
   "source": [
    "# Safe parameterized queries - avoid SQL injection!\n",
    "\n",
    "def get_customer_history(customer_id, min_revenue=0):\n",
    "    \"\"\"\n",
    "    Safely query customer transaction history\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        t.transaction_id,\n",
    "        t.date,\n",
    "        p.product_name,\n",
    "        p.category,\n",
    "        t.revenue\n",
    "    FROM transactions t\n",
    "    JOIN products p ON t.product_id = p.product_id\n",
    "    WHERE t.customer_id = ?\n",
    "      AND t.revenue > ?\n",
    "    ORDER BY t.date DESC\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use parameterized query for safety\n",
    "    return pd.read_sql(query, conn, params=(customer_id, min_revenue))\n",
    "\n",
    "# Example usage\n",
    "customer_data = get_customer_history(customer_id=42, min_revenue=100)\n",
    "print(\"üîí Safe parameterized query result:\")\n",
    "print(customer_data)\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è NEVER use string formatting for SQL queries - always use parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Pushing Pandas Operations to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach 1: Load all data, filter in pandas (DON'T DO THIS)\n",
      "```python\n",
      "# This loads ALL data into memory first!\n",
      "df = pd.read_sql('SELECT * FROM transactions', conn)\n",
      "df_filtered = df[(df['store_id'].isin(stores)) & (df['date'] >= start)]\n",
      "```\n",
      "\n",
      "Approach 2: Filter in SQL (DO THIS)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "\n",
       "SELECT *\n",
       "FROM transactions\n",
       "WHERE store_id IN ('store_name','store_name')\n",
       "  AND date >= 'store_name'\n",
       "  AND date <= 'store_name'\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Loaded only 205 relevant rows instead of 10,000!\n"
     ]
    }
   ],
   "source": [
    "# Sometimes it's better to push operations to the database\n",
    "\n",
    "# Scenario: Complex filtering that could be done in either tool\n",
    "stores_of_interest = ['NYC', 'LA']\n",
    "date_range = ('2024-01-01', '2024-01-07')\n",
    "\n",
    "print(\"Approach 1: Load all data, filter in pandas (DON'T DO THIS)\")\n",
    "print(\"```python\")\n",
    "print(\"# This loads ALL data into memory first!\")\n",
    "print(\"df = pd.read_sql('SELECT * FROM transactions', conn)\")\n",
    "print(\"df_filtered = df[(df['store_id'].isin(stores)) & (df['date'] >= start)]\")\n",
    "print(\"```\\n\")\n",
    "\n",
    "print(\"Approach 2: Filter in SQL (DO THIS)\")\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM transactions\n",
    "WHERE store_id IN ({','.join(['?'] * len(stores_of_interest))})\n",
    "  AND date >= ?\n",
    "  AND date <= ?\n",
    "\"\"\"\n",
    "\n",
    "show_sql(query.replace('?', \"'store_name'\"))\n",
    "\n",
    "# Execute with parameters\n",
    "filtered_data = pd.read_sql(\n",
    "    query, \n",
    "    conn, \n",
    "    params=stores_of_interest + list(date_range)\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded only {len(filtered_data):,} relevant rows instead of {len(transactions):,}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Practice Exercises\n",
    "\n",
    "Now it's your turn! Complete these exercises using BOTH pandas and SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Customer Segmentation\n",
    "\n",
    "Find the top 10% of customers by total spending and analyze their behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêº Your pandas solution:\n",
      "Total customers: 1498.\n",
      "There are 150 customers with total spending higher than 90th percentile of revenue ($6791.655)\n",
      "customer_id\n",
      "277    11874.50\n",
      "700    11816.10\n",
      "798    11360.77\n",
      "1257   10669.33\n",
      "849    10524.02\n",
      "         ...   \n",
      "793     6821.31\n",
      "859     6808.82\n",
      "1136    6804.27\n",
      "988     6802.18\n",
      "1438    6799.39\n",
      "Name: revenue, Length: 150, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# TODO: Your pandas solution here\n",
    "# Hint: Use quantile() to find the 90th percentile threshold\n",
    "\n",
    "# pandas_solution = ...\n",
    "unique_customer = len(transactions['customer_id'].unique())\n",
    "customers_grouped = transactions.groupby('customer_id')['revenue'].sum()\n",
    "customers_90th_percentile = customers_grouped[customers_grouped>customers_grouped.quantile(0.9)]\n",
    "\n",
    "print(\"üêº Your pandas solution:\")\n",
    "print(f\"Total customers: {unique_customer}.\")\n",
    "print(f\"There are {len(customers_90th_percentile)} customers with total spending higher than 90th percentile of revenue (${customers_grouped.quantile(0.9)})\")\n",
    "print(customers_90th_percentile.sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóÑÔ∏è Your SQL solution:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "\n",
       "WITH total_spend_customer AS (\n",
       "    SELECT \n",
       "        customer_id,\n",
       "        SUM(revenue) as sum_revenue\n",
       "    FROM transactions\n",
       "    GROUP BY customer_id\n",
       "),\n",
       "top10 AS (\n",
       "    SELECT\n",
       "        customer_id, sum_revenue,\n",
       "        NTILE(10) OVER (ORDER BY sum_revenue) AS decile\n",
       "    FROM total_spend_customer\n",
       ")\n",
       "SELECT\n",
       "    customer_id, sum_revenue,\n",
       "    decile as P90_total_spend   \n",
       "FROM top10\n",
       "WHERE decile = 10\n",
       "ORDER BY sum_revenue DESC\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "     customer_id  sum_revenue  P90_total_spend\n",
      "0            277     11874.50               10\n",
      "1            700     11816.10               10\n",
      "2            798     11360.77               10\n",
      "3           1257     10669.33               10\n",
      "4            849     10524.02               10\n",
      "..           ...          ...              ...\n",
      "144          611      6835.58               10\n",
      "145          793      6821.31               10\n",
      "146          859      6808.82               10\n",
      "147         1136      6804.27               10\n",
      "148          988      6802.18               10\n",
      "\n",
      "[149 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Your SQL solution here\n",
    "# Hint: Use NTILE() or calculate percentiles with window functions\n",
    "\n",
    "sql_query = \"\"\"\n",
    "WITH total_spend_customer AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        SUM(revenue) as sum_revenue\n",
    "    FROM transactions\n",
    "    GROUP BY customer_id\n",
    "),\n",
    "top10 AS (\n",
    "    SELECT\n",
    "        customer_id, sum_revenue,\n",
    "        NTILE(10) OVER (ORDER BY sum_revenue) AS decile\n",
    "    FROM total_spend_customer\n",
    ")\n",
    "SELECT\n",
    "    customer_id, sum_revenue,\n",
    "    decile as P90_total_spend   \n",
    "FROM top10\n",
    "WHERE decile = 10\n",
    "ORDER BY sum_revenue DESC\n",
    "\"\"\"\n",
    "print(\"üóÑÔ∏è Your SQL solution:\")\n",
    "show_sql(sql_query)\n",
    "\n",
    "sql_solution = pd.read_sql(sql_query, conn)\n",
    "print(\"Output:\")\n",
    "print(sql_solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Cohort Analysis\n",
    "\n",
    "Calculate retention by customer signup month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a cohort analysis\n",
    "# 1. Group customers by signup month\n",
    "# 2. Track their activity in subsequent months\n",
    "# 3. Calculate retention rates\n",
    "\n",
    "# Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêº Your pandas solution:\n",
      "1) Create signup_month column in customers table.\n",
      "2) Left merge transactions table with customers table, on customer_id.\n",
      "3) Compute months between signup and transaction date, create a new column months_since_signup.\n",
      "4) Find active customers: filter out the rows where months_since_signup >= 0, drop duplicates.\n",
      "4a) Group the customer by signup_month and months_since_signup.\n",
      "4b) Find the count of active customers at most recent 'months_since_signup'.\n",
      "5) Find the original count of customers during signup.\n",
      "5a) Merge the customer signup_count to the active customer table.\n",
      "5b) Compute the retention: latest active customer / original signup.\n",
      "\n",
      "Final output:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>signup_month</th>\n",
       "      <th>months_since_signup</th>\n",
       "      <th>latest_active_count</th>\n",
       "      <th>signup_count</th>\n",
       "      <th>retention_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>15</td>\n",
       "      <td>65</td>\n",
       "      <td>124</td>\n",
       "      <td>52.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02</td>\n",
       "      <td>14</td>\n",
       "      <td>65</td>\n",
       "      <td>112</td>\n",
       "      <td>58.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03</td>\n",
       "      <td>13</td>\n",
       "      <td>67</td>\n",
       "      <td>124</td>\n",
       "      <td>54.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-04</td>\n",
       "      <td>12</td>\n",
       "      <td>67</td>\n",
       "      <td>120</td>\n",
       "      <td>55.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05</td>\n",
       "      <td>11</td>\n",
       "      <td>64</td>\n",
       "      <td>124</td>\n",
       "      <td>51.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-06</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>120</td>\n",
       "      <td>53.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-07</td>\n",
       "      <td>9</td>\n",
       "      <td>79</td>\n",
       "      <td>124</td>\n",
       "      <td>63.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-08</td>\n",
       "      <td>8</td>\n",
       "      <td>75</td>\n",
       "      <td>124</td>\n",
       "      <td>60.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-09</td>\n",
       "      <td>7</td>\n",
       "      <td>70</td>\n",
       "      <td>120</td>\n",
       "      <td>58.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-10</td>\n",
       "      <td>6</td>\n",
       "      <td>71</td>\n",
       "      <td>124</td>\n",
       "      <td>57.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-11</td>\n",
       "      <td>5</td>\n",
       "      <td>67</td>\n",
       "      <td>120</td>\n",
       "      <td>55.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-12</td>\n",
       "      <td>4</td>\n",
       "      <td>75</td>\n",
       "      <td>124</td>\n",
       "      <td>60.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2024-01</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>40</td>\n",
       "      <td>60.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   signup_month  months_since_signup  latest_active_count  signup_count  \\\n",
       "0       2023-01                   15                   65           124   \n",
       "1       2023-02                   14                   65           112   \n",
       "2       2023-03                   13                   67           124   \n",
       "3       2023-04                   12                   67           120   \n",
       "4       2023-05                   11                   64           124   \n",
       "5       2023-06                   10                   64           120   \n",
       "6       2023-07                    9                   79           124   \n",
       "7       2023-08                    8                   75           124   \n",
       "8       2023-09                    7                   70           120   \n",
       "9       2023-10                    6                   71           124   \n",
       "10      2023-11                    5                   67           120   \n",
       "11      2023-12                    4                   75           124   \n",
       "12      2024-01                    3                   24            40   \n",
       "\n",
       "    retention_rate  \n",
       "0            52.42  \n",
       "1            58.04  \n",
       "2            54.03  \n",
       "3            55.83  \n",
       "4            51.61  \n",
       "5            53.33  \n",
       "6            63.71  \n",
       "7            60.48  \n",
       "8            58.33  \n",
       "9            57.26  \n",
       "10           55.83  \n",
       "11           60.48  \n",
       "12           60.00  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"üêº Your pandas solution:\")\n",
    "print(f\"1) Create signup_month column in customers table.\")\n",
    "\n",
    "customers['signup_month'] = customers['signup_date'].dt.to_period('M')\n",
    "\n",
    "print(f\"2) Left merge transactions table with customers table, on customer_id.\")\n",
    "\n",
    "merged = pd.merge(\n",
    "    transactions[['transaction_id', 'customer_id', 'date','revenue']],\n",
    "    customers[['customer_id', 'signup_date', 'signup_month']],\n",
    "    on = 'customer_id',\n",
    "    how= 'left'\n",
    ")\n",
    "\n",
    "print(f\"3) Compute months between signup and transaction date, create a new column months_since_signup.\")\n",
    "\n",
    "merged['months_since_signup']=((merged['date'].dt.year - merged['signup_date'].dt.year)*12 + (merged['date'].dt.month-merged['signup_date'].dt.month)).astype('Int64')\n",
    "\n",
    "print(f\"4) Find active customers: filter out the rows where months_since_signup >= 0, drop duplicates.\")\n",
    "\n",
    "active_customers = merged[merged['months_since_signup']>= 0][['customer_id','signup_month','months_since_signup']].drop_duplicates()\n",
    "\n",
    "print(f\"4a) Group the customer by signup_month and months_since_signup.\")\n",
    "print(f\"4b) Find the count of active customers at most recent 'months_since_signup'.\")\n",
    "\n",
    "active_customers_grouped = active_customers.groupby(['signup_month','months_since_signup'])['customer_id'].nunique().reset_index()\n",
    "active_customers_recent_idx = active_customers_grouped.groupby('signup_month')['months_since_signup'].idxmax()\n",
    "active_customers_recent = active_customers_grouped.loc[active_customers_recent_idx].rename(columns = {'customer_id':'latest_active_count'})\n",
    "\n",
    "print(f\"5) Find the original count of customers during signup.\")\n",
    "print(f\"5a) Merge the customer signup_count to the active customer table.\")\n",
    "print(f\"5b) Compute the retention: latest active customer / original signup.\")\n",
    "print(f\"\\nFinal output:\\n\")\n",
    "\n",
    "signup_count = customers.groupby('signup_month')['customer_id'].nunique().reset_index().rename(columns = {'customer_id':'signup_count'})\n",
    "retention = pd.merge(\n",
    "    active_customers_recent,\n",
    "    signup_count,\n",
    "    on = 'signup_month',\n",
    "    how = 'left'\n",
    ")\n",
    "retention['retention_rate'] = round(retention['latest_active_count'] / retention['signup_count'] * 100, 2)\n",
    "retention     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóÑÔ∏è Your SQL solution:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "\n",
       "WITH base AS (\n",
       "    SELECT \n",
       "        COUNT (DISTINCT t.customer_id) AS active_customers,\n",
       "        date(t.date, 'start of month') AS transaction_month,\n",
       "        date(c.signup_date, 'start of month') AS signup_month,\n",
       "        (CAST(strftime('%Y', t.date) AS INT) * 12 + CAST(strftime('%m', t.date) AS INT)\n",
       "      - CAST(strftime('%Y', c.signup_date) AS INT) * 12 - CAST(strftime('%m', c.signup_date) AS INT)    \n",
       "        ) AS months_since_signup\n",
       "    FROM transactions t\n",
       "    LEFT JOIN customers c \n",
       "    ON t.customer_id = c.customer_id\n",
       "    GROUP BY signup_month, months_since_signup\n",
       "),\n",
       "recent AS (\n",
       "    SELECT *,\n",
       "        ROW_NUMBER() OVER (PARTITION BY signup_month ORDER BY months_since_signup DESC) AS recent_flag\n",
       "    FROM base\n",
       "),\n",
       "signup AS (\n",
       "    SELECT \n",
       "        COUNT(*) AS signup_count,\n",
       "        date(signup_date, 'start of month') AS signup_month\n",
       "    FROM customers\n",
       "    GROUP BY strftime('%Y-%m', signup_date)\n",
       ")\n",
       "SELECT \n",
       "    r.signup_month, r.transaction_month as latest_transaction_month, r.months_since_signup, r.active_customers,\n",
       "    s.signup_count,\n",
       "    ROUND(100.0 * r.active_customers / NULLIF(s.signup_count,0), 2) AS retention_percentage\n",
       "FROM recent r\n",
       "LEFT JOIN signup s ON r.signup_month = s.signup_month\n",
       "WHERE recent_flag = 1\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>signup_month</th>\n",
       "      <th>latest_transaction_month</th>\n",
       "      <th>months_since_signup</th>\n",
       "      <th>active_customers</th>\n",
       "      <th>signup_count</th>\n",
       "      <th>retention_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>15</td>\n",
       "      <td>65</td>\n",
       "      <td>124</td>\n",
       "      <td>52.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-01</td>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>14</td>\n",
       "      <td>65</td>\n",
       "      <td>112</td>\n",
       "      <td>58.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>13</td>\n",
       "      <td>67</td>\n",
       "      <td>124</td>\n",
       "      <td>54.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-04-01</td>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>12</td>\n",
       "      <td>67</td>\n",
       "      <td>120</td>\n",
       "      <td>55.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>11</td>\n",
       "      <td>64</td>\n",
       "      <td>124</td>\n",
       "      <td>51.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-06-01</td>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>120</td>\n",
       "      <td>53.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>9</td>\n",
       "      <td>79</td>\n",
       "      <td>124</td>\n",
       "      <td>63.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-08-01</td>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>8</td>\n",
       "      <td>75</td>\n",
       "      <td>124</td>\n",
       "      <td>60.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-09-01</td>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>7</td>\n",
       "      <td>70</td>\n",
       "      <td>120</td>\n",
       "      <td>58.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-10-01</td>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>6</td>\n",
       "      <td>71</td>\n",
       "      <td>124</td>\n",
       "      <td>57.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>5</td>\n",
       "      <td>67</td>\n",
       "      <td>120</td>\n",
       "      <td>55.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>75</td>\n",
       "      <td>124</td>\n",
       "      <td>60.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>40</td>\n",
       "      <td>60.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   signup_month latest_transaction_month  months_since_signup  \\\n",
       "0    2023-01-01               2024-04-01                   15   \n",
       "1    2023-02-01               2024-04-01                   14   \n",
       "2    2023-03-01               2024-04-01                   13   \n",
       "3    2023-04-01               2024-04-01                   12   \n",
       "4    2023-05-01               2024-04-01                   11   \n",
       "5    2023-06-01               2024-04-01                   10   \n",
       "6    2023-07-01               2024-04-01                    9   \n",
       "7    2023-08-01               2024-04-01                    8   \n",
       "8    2023-09-01               2024-04-01                    7   \n",
       "9    2023-10-01               2024-04-01                    6   \n",
       "10   2023-11-01               2024-04-01                    5   \n",
       "11   2023-12-01               2024-04-01                    4   \n",
       "12   2024-01-01               2024-04-01                    3   \n",
       "\n",
       "    active_customers  signup_count  retention_percentage  \n",
       "0                 65           124                 52.42  \n",
       "1                 65           112                 58.04  \n",
       "2                 67           124                 54.03  \n",
       "3                 67           120                 55.83  \n",
       "4                 64           124                 51.61  \n",
       "5                 64           120                 53.33  \n",
       "6                 79           124                 63.71  \n",
       "7                 75           124                 60.48  \n",
       "8                 70           120                 58.33  \n",
       "9                 71           124                 57.26  \n",
       "10                67           120                 55.83  \n",
       "11                75           124                 60.48  \n",
       "12                24            40                 60.00  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQL solution\n",
    "\n",
    "sql_query_ex2 = \"\"\"\n",
    "WITH base AS (\n",
    "    SELECT \n",
    "        COUNT (DISTINCT t.customer_id) AS active_customers,\n",
    "        date(t.date, 'start of month') AS transaction_month,\n",
    "        date(c.signup_date, 'start of month') AS signup_month,\n",
    "        (CAST(strftime('%Y', t.date) AS INT) * 12 + CAST(strftime('%m', t.date) AS INT)\n",
    "      - CAST(strftime('%Y', c.signup_date) AS INT) * 12 - CAST(strftime('%m', c.signup_date) AS INT)    \n",
    "        ) AS months_since_signup\n",
    "    FROM transactions t\n",
    "    LEFT JOIN customers c \n",
    "    ON t.customer_id = c.customer_id\n",
    "    GROUP BY signup_month, months_since_signup\n",
    "),\n",
    "recent AS (\n",
    "    SELECT *,\n",
    "        ROW_NUMBER() OVER (PARTITION BY signup_month ORDER BY months_since_signup DESC) AS recent_flag\n",
    "    FROM base\n",
    "),\n",
    "signup AS (\n",
    "    SELECT \n",
    "        COUNT(*) AS signup_count,\n",
    "        date(signup_date, 'start of month') AS signup_month\n",
    "    FROM customers\n",
    "    GROUP BY strftime('%Y-%m', signup_date)\n",
    ")\n",
    "SELECT \n",
    "    r.signup_month, r.transaction_month as latest_transaction_month, r.months_since_signup, r.active_customers,\n",
    "    s.signup_count,\n",
    "    ROUND(100.0 * r.active_customers / NULLIF(s.signup_count,0), 2) AS retention_percentage\n",
    "FROM recent r\n",
    "LEFT JOIN signup s ON r.signup_month = s.signup_month\n",
    "WHERE recent_flag = 1\n",
    "\"\"\"\n",
    "print(\"üóÑÔ∏è Your SQL solution:\")\n",
    "show_sql(sql_query_ex2)\n",
    "sql_ex2_result = pd.read_sql(sql_query_ex2,conn)\n",
    "sql_ex2_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Product Affinity\n",
    "\n",
    "Find which products are frequently bought together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement market basket analysis\n",
    "# Find products that appear in the same transactions\n",
    "\n",
    "# Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêº Your pandas solution:\n",
      "1) Find transactions on the same date by the same customers.\n",
      "2) Merge transactions with same_day_transactions, filter to rows where unique_product_count is > 1.\n",
      "3) Group by date and customer_id, aggregate the list of product_id.\n",
      "\n",
      "Output from step 2:\n",
      "\n",
      "      date_only  customer_id  product_id  quantity\n",
      "24   2024-01-01          956         158         4\n",
      "64   2024-01-01          956         165         2\n",
      "40   2024-01-01          958          47         2\n",
      "42   2024-01-01          958          66         4\n",
      "3    2024-01-01         1295         158         3\n",
      "...         ...          ...         ...       ...\n",
      "9931 2024-04-13         1089         141         1\n",
      "9913 2024-04-13         1130         112         1\n",
      "9978 2024-04-13         1130         166         1\n",
      "9959 2024-04-13         1154         142         3\n",
      "9976 2024-04-13         1154         125         2\n",
      "\n",
      "[601 rows x 4 columns]\n",
      "\n",
      "Output from step 3:\n",
      "\n",
      "date_only   customer_id\n",
      "2024-01-01  956            [158, 165]\n",
      "            958              [47, 66]\n",
      "            1295            [158, 18]\n",
      "2024-01-02  231            [175, 126]\n",
      "            503              [94, 73]\n",
      "                              ...    \n",
      "2024-04-12  700            [142, 140]\n",
      "2024-04-13  317             [146, 69]\n",
      "            1089             [7, 141]\n",
      "            1130           [112, 166]\n",
      "            1154           [142, 125]\n",
      "Name: product_id, Length: 298, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"üêº Your pandas solution:\")\n",
    "\n",
    "print(f\"1) Find transactions on the same date by the same customers.\")\n",
    "\n",
    "transactions['date_only']=pd.to_datetime(transactions['date_only'])\n",
    "same_day_transactions = transactions.groupby(['date_only','customer_id'])['product_id'].nunique().reset_index()\n",
    "same_day_transactions = same_day_transactions.rename(columns={'product_id':'unique_product_count'})\n",
    "\n",
    "print(f\"2) Merge transactions with same_day_transactions, filter to rows where unique_product_count is > 1.\")\n",
    "\n",
    "product_purchase = pd.merge(\n",
    "    transactions[['date_only','customer_id', 'product_id', 'quantity']],\n",
    "    same_day_transactions[['date_only', 'customer_id', 'unique_product_count']],\n",
    "    on = ['date_only','customer_id'],\n",
    "    how = 'left')\n",
    "product_purchase_multiple = product_purchase[product_purchase['unique_product_count']>1]\n",
    "product_purchase_multiple = product_purchase_multiple.sort_values(['date_only','customer_id']).drop('unique_product_count', axis=1)\n",
    "\n",
    "print(f\"3) Group by date and customer_id, aggregate the list of product_id.\")\n",
    "\n",
    "product_purchase_grouped = product_purchase_multiple.groupby(['date_only','customer_id'])['product_id'].agg(list)\n",
    "\n",
    "print(f\"\\nOutput from step 2:\\n\")\n",
    "print(product_purchase_multiple)\n",
    "\n",
    "print(f\"\\nOutput from step 3:\\n\")\n",
    "print(product_purchase_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "\n",
       "WITH base AS (\n",
       "    SELECT \n",
       "        date_only, customer_id, product_id\n",
       "    FROM transactions\n",
       "),\n",
       "same_day AS (\n",
       "    SELECT\n",
       "        date_only, customer_id,\n",
       "        COUNT(DISTINCT product_id) AS unique_product_count    \n",
       "    FROM base\n",
       "    GROUP BY date_only, customer_id\n",
       "),\n",
       "multiple_purchase AS (\n",
       "    SELECT \n",
       "        b.date_only, b.customer_id, b.product_id\n",
       "    FROM base b\n",
       "    JOIN same_day s \n",
       "    ON b.date_only = s.date_only AND b.customer_id = s.customer_id\n",
       "    WHERE s.unique_product_count > 1\n",
       ")\n",
       "SELECT *\n",
       "FROM multiple_purchase\n",
       "ORDER BY date_only, customer_id, product_id\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_only</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>956</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>956</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>958</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>958</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>1295</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>2024-04-13</td>\n",
       "      <td>1089</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>2024-04-13</td>\n",
       "      <td>1130</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>2024-04-13</td>\n",
       "      <td>1130</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>2024-04-13</td>\n",
       "      <td>1154</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>2024-04-13</td>\n",
       "      <td>1154</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>601 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      date_only  customer_id  product_id\n",
       "0    2024-01-01          956         158\n",
       "1    2024-01-01          956         165\n",
       "2    2024-01-01          958          47\n",
       "3    2024-01-01          958          66\n",
       "4    2024-01-01         1295          18\n",
       "..          ...          ...         ...\n",
       "596  2024-04-13         1089         141\n",
       "597  2024-04-13         1130         112\n",
       "598  2024-04-13         1130         166\n",
       "599  2024-04-13         1154         125\n",
       "600  2024-04-13         1154         142\n",
       "\n",
       "[601 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQL solution\n",
    "# option 1: output table is similar to panda's output step 2\n",
    "\n",
    "sql_query_ex4 = \"\"\"\n",
    "WITH base AS (\n",
    "    SELECT \n",
    "        date_only, customer_id, product_id\n",
    "    FROM transactions\n",
    "),\n",
    "same_day AS (\n",
    "    SELECT\n",
    "        date_only, customer_id,\n",
    "        COUNT(DISTINCT product_id) AS unique_product_count    \n",
    "    FROM base\n",
    "    GROUP BY date_only, customer_id\n",
    "),\n",
    "multiple_purchase AS (\n",
    "    SELECT \n",
    "        b.date_only, b.customer_id, b.product_id\n",
    "    FROM base b\n",
    "    JOIN same_day s \n",
    "    ON b.date_only = s.date_only AND b.customer_id = s.customer_id\n",
    "    WHERE s.unique_product_count > 1\n",
    ")\n",
    "SELECT *\n",
    "FROM multiple_purchase\n",
    "ORDER BY date_only, customer_id, product_id\n",
    "\"\"\"\n",
    "show_sql(sql_query_ex4)\n",
    "sql_result_ex4 = pd.read_sql(sql_query_ex4,conn)\n",
    "sql_result_ex4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "\n",
       "WITH base AS (\n",
       "    SELECT \n",
       "        date_only, customer_id, product_id\n",
       "    FROM transactions\n",
       "),\n",
       "same_day AS (\n",
       "    SELECT\n",
       "        date_only, customer_id,\n",
       "        COUNT(DISTINCT product_id) AS unique_product_count    \n",
       "    FROM base\n",
       "    GROUP BY date_only, customer_id\n",
       "),\n",
       "multiple_purchase AS (\n",
       "    SELECT \n",
       "        b.date_only, b.customer_id, b.product_id\n",
       "    FROM base b\n",
       "    JOIN same_day s \n",
       "    ON b.date_only = s.date_only AND b.customer_id = s.customer_id\n",
       "    WHERE s.unique_product_count > 1\n",
       ")\n",
       "SELECT\n",
       "    date_only,\n",
       "    customer_id,\n",
       "    group_concat(product_id, ',') AS product_ids\n",
       "FROM multiple_purchase\n",
       "GROUP BY date_only, customer_id\n",
       "ORDER BY date_only, customer_id\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_only</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>956</td>\n",
       "      <td>158,165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>958</td>\n",
       "      <td>47,66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>1295</td>\n",
       "      <td>158,18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>231</td>\n",
       "      <td>175,126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>503</td>\n",
       "      <td>94,73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>2024-04-12</td>\n",
       "      <td>700</td>\n",
       "      <td>142,140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>2024-04-13</td>\n",
       "      <td>317</td>\n",
       "      <td>146,69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>2024-04-13</td>\n",
       "      <td>1089</td>\n",
       "      <td>7,141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>2024-04-13</td>\n",
       "      <td>1130</td>\n",
       "      <td>112,166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>2024-04-13</td>\n",
       "      <td>1154</td>\n",
       "      <td>142,125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>298 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      date_only  customer_id product_ids\n",
       "0    2024-01-01          956     158,165\n",
       "1    2024-01-01          958       47,66\n",
       "2    2024-01-01         1295      158,18\n",
       "3    2024-01-02          231     175,126\n",
       "4    2024-01-02          503       94,73\n",
       "..          ...          ...         ...\n",
       "293  2024-04-12          700     142,140\n",
       "294  2024-04-13          317      146,69\n",
       "295  2024-04-13         1089       7,141\n",
       "296  2024-04-13         1130     112,166\n",
       "297  2024-04-13         1154     142,125\n",
       "\n",
       "[298 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# option 2: output table is similar to panda's output step 3\n",
    "sql_query_ex3 = \"\"\"\n",
    "WITH base AS (\n",
    "    SELECT \n",
    "        date_only, customer_id, product_id\n",
    "    FROM transactions\n",
    "),\n",
    "same_day AS (\n",
    "    SELECT\n",
    "        date_only, customer_id,\n",
    "        COUNT(DISTINCT product_id) AS unique_product_count    \n",
    "    FROM base\n",
    "    GROUP BY date_only, customer_id\n",
    "),\n",
    "multiple_purchase AS (\n",
    "    SELECT \n",
    "        b.date_only, b.customer_id, b.product_id\n",
    "    FROM base b\n",
    "    JOIN same_day s \n",
    "    ON b.date_only = s.date_only AND b.customer_id = s.customer_id\n",
    "    WHERE s.unique_product_count > 1\n",
    ")\n",
    "SELECT\n",
    "    date_only,\n",
    "    customer_id,\n",
    "    group_concat(product_id, ',') AS product_ids\n",
    "FROM multiple_purchase\n",
    "GROUP BY date_only, customer_id\n",
    "ORDER BY date_only, customer_id\n",
    "\"\"\"\n",
    "show_sql(sql_query_ex3)\n",
    "sql_result_ex3 = pd.read_sql(sql_query_ex3,conn)\n",
    "sql_result_ex3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Key Takeaways\n",
    "\n",
    "1. **Mental Model Mapping**:\n",
    "   - `df.groupby()` ‚Üí `GROUP BY`\n",
    "   - `df.merge()` ‚Üí `JOIN`\n",
    "   - `df['col'].rank()` ‚Üí `RANK() OVER()`\n",
    "   - `df['col'].shift()` ‚Üí `LAG()/LEAD()`\n",
    "\n",
    "2. **When to Use SQL**:\n",
    "   - Data is in a database (avoid loading unnecessary data)\n",
    "   - Need to share queries with non-Python users\n",
    "   - Production pipelines requiring audit trails\n",
    "   - Working with data larger than memory\n",
    "\n",
    "3. **When to Use Pandas**:\n",
    "   - Exploratory data analysis\n",
    "   - Complex statistical operations\n",
    "   - Data cleaning and string manipulation\n",
    "   - Visualization preparation\n",
    "\n",
    "4. **Best Practices**:\n",
    "   - Use SQL to reduce data volume first\n",
    "   - Always use parameterized queries\n",
    "   - Think in sets (SQL) vs iterations (pandas)\n",
    "   - Document complex queries with CTEs\n",
    "   - Profile performance for large datasets\n",
    "\n",
    "5. **Hybrid Approach**:\n",
    "   - SQL for extraction and reduction\n",
    "   - Pandas for transformation and analysis\n",
    "   - SQL for productionization\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "In the next notebook, we'll dive deeper into:\n",
    "- Data warehouse design patterns\n",
    "- Star and snowflake schemas\n",
    "- Optimizing query performance\n",
    "- Working with cloud data warehouses\n",
    "\n",
    "Remember: **You don't choose pandas OR SQL - you master BOTH!** üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Database connection closed. Great work!\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "conn.close()\n",
    "print(\"‚úÖ Database connection closed. Great work!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
